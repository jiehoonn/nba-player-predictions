{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 01: Data Collection\n",
    "\n",
    "## Objective\n",
    "Collect and prepare clean NBA data for player performance prediction.\n",
    "\n",
    "## Data Pipeline\n",
    "1. **data/raw/**: Original NBA API data (unmodified)\n",
    "2. **data/processed/**: Cleaned and enriched data ready for feature engineering\n",
    "\n",
    "## What This Notebook Does\n",
    "1. **Collects raw data** from NBA API (game logs, shot charts, team stats)\n",
    "2. **Enriches data** with opponent stats, team context, rest days\n",
    "3. **Cleans data** (removes duplicates, orphans, standardizes naming)\n",
    "4. **Saves to processed/** for use in feature engineering\n",
    "\n",
    "## Output Files (data/processed/)\n",
    "- `gamelogs_combined.parquet` - ~73,000 games with opponent/team context\n",
    "- `shot_charts_all.parquet` - ~631,000 shots (deduplicated)\n",
    "- Per-season game log files\n",
    "\n",
    "## Key Features Added\n",
    "- **Opponent stats**: DEF_RATING, OFF_RATING, PACE, W_PCT\n",
    "- **Team stats**: Player's own team context  \n",
    "- **Rest days**: DAYS_REST (-1 = first game for player, otherwise days since last game), IS_B2B\n",
    "- **Clean data**: No duplicates, no orphaned records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete - imported NBA API and configured paths\n",
      "   data/raw/       - for unmodified API data\n",
      "   data/processed/ - for cleaned and enriched data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from nba_api.stats.endpoints import leaguegamelog, playergamelog, shotchartdetail\n",
    "\n",
    "SEASONS = ['2019-20', '2020-21', '2021-22', '2022-23', '2023-24']\n",
    "N_PLAYERS = 200\n",
    "RATE_LIMIT = 0.6\n",
    "\n",
    "# Create both raw and processed directories\n",
    "raw_path = Path('../data/raw')\n",
    "processed_path = Path('../data/processed')\n",
    "raw_path.mkdir(parents=True, exist_ok=True)\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Setup complete - imported NBA API and configured paths\")\n",
    "print(f\"   data/raw/       - for unmodified API data\")\n",
    "print(f\"   data/processed/ - for cleaned and enriched data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Collect Game Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Identifying top 200 players per season based on total minutes played...\n",
      "\n",
      "============================================================\n",
      "2019-20\n",
      "============================================================\n",
      "Top player: CJ McCollum (2560 min)\n",
      "Identified 200 players for this season\n",
      "============================================================\n",
      "2020-21\n",
      "============================================================\n",
      "Top player: Julius Randle (2666 min)\n",
      "Identified 200 players for this season\n",
      "============================================================\n",
      "2021-22\n",
      "============================================================\n",
      "Top player: Mikal Bridges (2854 min)\n",
      "Identified 200 players for this season\n",
      "============================================================\n",
      "2022-23\n",
      "============================================================\n",
      "Top player: Mikal Bridges (2965 min)\n",
      "Identified 200 players for this season\n",
      "============================================================\n",
      "2023-24\n",
      "============================================================\n",
      "Top player: DeMar DeRozan (2995 min)\n",
      "Identified 200 players for this season\n",
      "\n",
      "============================================================\n",
      "Step 2: Combining unique players across all seasons...\n",
      "============================================================\n",
      "\n",
      "Found 369 unique players across all 5 seasons\n",
      "\n",
      "============================================================\n",
      "Step 3: Collecting complete game logs for all identified players...\n",
      "This will take approximately 4-5 hours. Progress updates every 50 players.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  14%|█▎        | 50/369 [04:07<26:30,  4.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 50/369 players completed | 15,375 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  27%|██▋       | 100/369 [08:32<25:13,  5.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 100/369 players completed | 28,972 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  41%|████      | 150/369 [1:01:05<35:43,  9.79s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 150/369 players completed | 36,557 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  54%|█████▍    | 200/369 [2:01:22<20:18,  7.21s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 200/369 players completed | 42,159 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  68%|██████▊   | 250/369 [2:05:41<10:41,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 250/369 players completed | 54,356 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  81%|████████▏ | 300/369 [2:10:07<06:34,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 300/369 players completed | 66,869 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players:  95%|█████████▍| 350/369 [3:02:39<01:41,  5.36s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Progress: 350/369 players completed | 72,299 total games collected so far\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Players: 100%|██████████| 369/369 [3:39:24<00:00, 35.68s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Step 4: Combining all data and saving to disk...\n",
      "============================================================\n",
      "\n",
      "Games per season breakdown:\n",
      "  2019-20: 12,770 games\n",
      "  2020-21: 14,026 games\n",
      "  2021-22: 16,097 games\n",
      "  2022-23: 15,120 games\n",
      "  2023-24: 14,496 games\n",
      "\n",
      "Final totals: 72,509 games from 289 unique players\n",
      "Data saved to data/raw/ directory\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Identify top 200 players from each season\n",
    "print(\"Step 1: Identifying top 200 players per season based on total minutes played...\\n\")\n",
    "top_players_by_season = {}\n",
    "\n",
    "for season in SEASONS:\n",
    "    print(f\"{'='*60}\\n{season}\\n{'='*60}\")\n",
    "    \n",
    "    # Get league game log for this season\n",
    "    league_log = leaguegamelog.LeagueGameLog(\n",
    "        season=season,\n",
    "        season_type_all_star='Regular Season',\n",
    "        player_or_team_abbreviation='P'\n",
    "    )\n",
    "    df_league = league_log.get_data_frames()[0]\n",
    "    time.sleep(RATE_LIMIT)\n",
    "    \n",
    "    # Get top 200 players by total minutes\n",
    "    top_players = (\n",
    "        df_league.groupby(['PLAYER_ID', 'PLAYER_NAME'])['MIN']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "        .sort_values('MIN', ascending=False)\n",
    "        .head(N_PLAYERS)\n",
    "    )\n",
    "    \n",
    "    top_players_by_season[season] = top_players\n",
    "    print(f\"Top player: {top_players.iloc[0]['PLAYER_NAME']} ({top_players.iloc[0]['MIN']:.0f} min)\")\n",
    "    print(f\"Identified {len(top_players)} players for this season\")\n",
    "\n",
    "# Step 2: Get ALL unique players across all seasons\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 2: Combining unique players across all seasons...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "all_top_players = pd.concat(top_players_by_season.values(), ignore_index=True)\n",
    "unique_players = all_top_players.drop_duplicates(subset='PLAYER_ID')[['PLAYER_ID', 'PLAYER_NAME']]\n",
    "print(f\"Found {len(unique_players)} unique players across all {len(SEASONS)} seasons\")\n",
    "\n",
    "# Step 3: Collect game logs for ALL seasons for each unique player\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 3: Collecting complete game logs for all identified players...\")\n",
    "print(f\"This will take approximately 4-5 hours. Progress updates every 50 players.\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "all_games = []\n",
    "for idx, (_, player) in enumerate(tqdm(unique_players.iterrows(), total=len(unique_players), desc=\"Players\")):\n",
    "    player_id = player['PLAYER_ID']\n",
    "    player_name = player['PLAYER_NAME']\n",
    "    \n",
    "    for season in SEASONS:\n",
    "        try:\n",
    "            gamelog = playergamelog.PlayerGameLog(\n",
    "                player_id=player_id, \n",
    "                season=season, \n",
    "                season_type_all_star='Regular Season'\n",
    "            )\n",
    "            df_games = gamelog.get_data_frames()[0]\n",
    "            \n",
    "            if len(df_games) > 0:\n",
    "                df_games['PLAYER_NAME'] = player_name\n",
    "                all_games.append(df_games)\n",
    "            \n",
    "            time.sleep(RATE_LIMIT)\n",
    "        except Exception as e:\n",
    "            # Player didn't play in this season - skip silently\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # Progress update every 50 players\n",
    "    if (idx + 1) % 50 == 0:\n",
    "        total_games = sum(len(g) for g in all_games)\n",
    "        print(f\"\\nProgress: {idx+1}/{len(unique_players)} players completed | {total_games:,} total games collected so far\")\n",
    "\n",
    "# Step 4: Combine and save\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"Step 4: Combining all data and saving to disk...\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "df_all = pd.concat(all_games, ignore_index=True)\n",
    "\n",
    "# Save combined file\n",
    "df_all.to_parquet(raw_path / \"gamelogs_combined.parquet\", index=False)\n",
    "\n",
    "# Also save per-season files for reference\n",
    "print(\"Games per season breakdown:\")\n",
    "for season in SEASONS:\n",
    "    season_data = df_all[df_all['SEASON_ID'].astype(str).str.contains(season.split('-')[0])]\n",
    "    if len(season_data) > 0:\n",
    "        season_data.to_parquet(raw_path / f\"gamelogs_{season}.parquet\", index=False)\n",
    "        print(f\"  {season}: {len(season_data):,} games\")\n",
    "\n",
    "print(f\"\\nFinal totals: {len(df_all):,} games from {df_all['Player_ID'].nunique()} unique players\")\n",
    "print(f\"Data saved to data/raw/ directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Collect Shot Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shot charts for 289 players...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 50/289 [29:28<49:24, 12.40s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 50/289 | Shots: 170,885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 100/289 [40:10<50:46, 16.12s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100/289 | Shots: 321,785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 150/289 [1:24:09<33:04, 14.28s/it]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 150/289 | Shots: 431,096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 200/289 [2:08:47<1:39:59, 67.41s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 200/289 | Shots: 453,461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 250/289 [2:57:46<05:37,  8.65s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 250/289 | Shots: 546,489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 289/289 [3:01:22<00:00, 37.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 597,106 shots successfully\n",
      "WARNING: 309 API errors occurred (expected for players without shots in some seasons)\n",
      "First 3 errors: [{'player_id': 203468, 'player_name': 'CJ McCollum', 'season': '2019-20', 'error': 'HTTPSConnectionPool(host=\\'stats.nba.com\\', port=443): Max retries exceeded with url: /stats/shotchartdetail?AheadBehind=&ClutchTime=&ContextFilter=&ContextMeasure=FGA&DateFrom=&DateTo=&EndPeriod=&EndRange=&GameID=&GameSegment=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&Period=0&PlayerID=203468&PlayerPosition=&PointDiff=&Position=&RangeType=&RookieYear=&Season=2019-20&SeasonSegment=&SeasonType=Regular+Season&StartPeriod=&StartRange=&TeamID=0&VsConference=&VsDivision= (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116a45810>: Failed to resolve \\'stats.nba.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'}, {'player_id': 203468, 'player_name': 'CJ McCollum', 'season': '2020-21', 'error': 'HTTPSConnectionPool(host=\\'stats.nba.com\\', port=443): Max retries exceeded with url: /stats/shotchartdetail?AheadBehind=&ClutchTime=&ContextFilter=&ContextMeasure=FGA&DateFrom=&DateTo=&EndPeriod=&EndRange=&GameID=&GameSegment=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&Period=0&PlayerID=203468&PlayerPosition=&PointDiff=&Position=&RangeType=&RookieYear=&Season=2020-21&SeasonSegment=&SeasonType=Regular+Season&StartPeriod=&StartRange=&TeamID=0&VsConference=&VsDivision= (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116a46c10>: Failed to resolve \\'stats.nba.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'}, {'player_id': 203468, 'player_name': 'CJ McCollum', 'season': '2021-22', 'error': 'HTTPSConnectionPool(host=\\'stats.nba.com\\', port=443): Max retries exceeded with url: /stats/shotchartdetail?AheadBehind=&ClutchTime=&ContextFilter=&ContextMeasure=FGA&DateFrom=&DateTo=&EndPeriod=&EndRange=&GameID=&GameSegment=&LastNGames=0&LeagueID=00&Location=&Month=0&OpponentTeamID=0&Outcome=&Period=0&PlayerID=203468&PlayerPosition=&PointDiff=&Position=&RangeType=&RookieYear=&Season=2021-22&SeasonSegment=&SeasonType=Regular+Season&StartPeriod=&StartRange=&TeamID=0&VsConference=&VsDivision= (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x116a474d0>: Failed to resolve \\'stats.nba.com\\' ([Errno 8] nodename nor servname provided, or not known)\"))'}]\n"
     ]
    }
   ],
   "source": [
    "players = df_all[['Player_ID', 'PLAYER_NAME']].drop_duplicates()\n",
    "print(f\"Collecting shot charts for {len(players)} players...\\n\")\n",
    "\n",
    "all_shots = []\n",
    "errors = []  # Track errors for debugging\n",
    "\n",
    "for idx, (_, row) in enumerate(tqdm(players.iterrows(), total=len(players))):\n",
    "    for season in SEASONS:\n",
    "        try:\n",
    "            shot_chart = shotchartdetail.ShotChartDetail(\n",
    "                team_id=0,\n",
    "                player_id=row['Player_ID'],\n",
    "                season_nullable=season,\n",
    "                season_type_all_star='Regular Season',\n",
    "                context_measure_simple='FGA'\n",
    "            )\n",
    "            df_shots_temp = shot_chart.get_data_frames()[0]\n",
    "            \n",
    "            if len(df_shots_temp) > 0:\n",
    "                # Add season identifier (API doesn't provide this)\n",
    "                df_shots_temp['Season'] = season\n",
    "                all_shots.append(df_shots_temp)\n",
    "                \n",
    "            time.sleep(RATE_LIMIT)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Log error but continue (player may not have played this season)\n",
    "            errors.append({\n",
    "                'player_id': row['Player_ID'],\n",
    "                'player_name': row['PLAYER_NAME'],\n",
    "                'season': season,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            time.sleep(1)\n",
    "    \n",
    "    if (idx + 1) % 50 == 0:\n",
    "        print(f\"Progress: {idx+1}/{len(players)} | Shots: {sum(len(s) for s in all_shots):,}\")\n",
    "\n",
    "df_shots = pd.concat(all_shots, ignore_index=True)\n",
    "df_shots.to_parquet(raw_path / \"shot_charts_all.parquet\", index=False)\n",
    "\n",
    "print(f\"\\nCollected {len(df_shots):,} shots successfully\")\n",
    "if len(errors) > 0:\n",
    "    print(f\"WARNING: {len(errors)} API errors occurred (expected for players without shots in some seasons)\")\n",
    "    print(f\"First 3 errors: {errors[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Collect Opponent/Team Stats\n",
    "\n",
    "Opponent strength is a critical contextual feature for prediction. We'll collect:\n",
    "- **Defensive Rating (DEFRTG)**: Points allowed per 100 possessions\n",
    "- **Offensive Rating (OFFRTG)**: Points scored per 100 possessions  \n",
    "- **Pace**: Possessions per 48 minutes\n",
    "- **Win/Loss record**: Team strength indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting team stats for all teams across all seasons...\n",
      "\n",
      "============================================================\n",
      "2019-20\n",
      "============================================================\n",
      "Collected stats for 30 teams\n",
      "============================================================\n",
      "2020-21\n",
      "============================================================\n",
      "Collected stats for 30 teams\n",
      "============================================================\n",
      "2021-22\n",
      "============================================================\n",
      "Collected stats for 30 teams\n",
      "============================================================\n",
      "2022-23\n",
      "============================================================\n",
      "Collected stats for 30 teams\n",
      "============================================================\n",
      "2023-24\n",
      "============================================================\n",
      "Collected stats for 30 teams\n",
      "\n",
      "Total: 150 team-season records\n",
      "Saved to data/raw/team_stats_all.parquet\n"
     ]
    }
   ],
   "source": [
    "from nba_api.stats.endpoints import leaguedashteamstats\n",
    "\n",
    "print(\"Collecting team stats for all teams across all seasons...\\n\")\n",
    "\n",
    "all_team_stats = []\n",
    "for season in SEASONS:\n",
    "    print(f\"{'='*60}\\n{season}\\n{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Get team stats for this season\n",
    "        team_stats = leaguedashteamstats.LeagueDashTeamStats(\n",
    "            season=season,\n",
    "            season_type_all_star='Regular Season',\n",
    "            measure_type_detailed_defense='Advanced',  # Get advanced stats (DRtg, ORtg, Pace)\n",
    "            per_mode_detailed='PerGame'\n",
    "        )\n",
    "        df_team_stats = team_stats.get_data_frames()[0]\n",
    "        \n",
    "        # Add season identifier\n",
    "        df_team_stats['SEASON'] = season\n",
    "        all_team_stats.append(df_team_stats)\n",
    "        \n",
    "        print(f\"Collected stats for {len(df_team_stats)} teams\")\n",
    "        time.sleep(RATE_LIMIT)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Error collecting team stats for {season}: {e}\")\n",
    "        time.sleep(1)\n",
    "\n",
    "# Combine all team stats\n",
    "df_team_stats_all = pd.concat(all_team_stats, ignore_index=True)\n",
    "\n",
    "# Save team stats\n",
    "df_team_stats_all.to_parquet(raw_path / \"team_stats_all.parquet\", index=False)\n",
    "print(f\"\\nTotal: {len(df_team_stats_all):,} team-season records\")\n",
    "print(f\"Saved to data/raw/team_stats_all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Merging Opponent Stats into Game Logs\n",
      "============================================================\n",
      "\n",
      "Merging opponent stats into game logs...\n",
      "   Game logs before merge: (72509, 31)\n",
      "   Team stats available: (150, 47)\n",
      "\n",
      "   Sample team names in team stats: ['Atlanta Hawks' 'Boston Celtics' 'Brooklyn Nets' 'Charlotte Hornets'\n",
      " 'Chicago Bulls']\n",
      "   Sample OPP_TEAM_NAME in game logs: ['Brooklyn Nets' 'Dallas Mavericks' 'Philadelphia 76ers' 'LA Clippers'\n",
      " 'Denver Nuggets']\n",
      "   Sample SEASON in team stats: ['2019-20' '2020-21' '2021-22' '2022-23' '2023-24']\n",
      "   Sample SEASON in game logs: ['2019-20' '2020-21' '2021-22' '2022-23' '2023-24']\n",
      "\n",
      "Merge complete!\n",
      "   Game logs after merge: (72509, 37)\n",
      "   SUCCESS: All 72,509 games have opponent stats\n",
      "\n",
      "Sample opponent stats:\n",
      "    MATCHUP OPP_TEAM_ABBREV  OPP_DEF_RATING  OPP_OFF_RATING  OPP_PACE\n",
      "  POR @ BKN             BKN           109.2           108.7    101.70\n",
      "  POR @ DAL             DAL           111.2           115.9     99.89\n",
      "POR vs. PHI             PHI           108.4           110.7     99.59\n",
      "\n",
      "Opponent-related columns added:\n",
      "   ['OPP_TEAM_ABBREV', 'OPP_TEAM_NAME', 'OPP_DEF_RATING', 'OPP_OFF_RATING', 'OPP_PACE', 'OPP_W', 'OPP_L', 'OPP_W_PCT']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Merging Opponent Stats into Game Logs\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# NBA team abbreviation to full name mapping\n",
    "TEAM_ABBREV_TO_NAME = {\n",
    "    'ATL': 'Atlanta Hawks',\n",
    "    'BKN': 'Brooklyn Nets',\n",
    "    'BOS': 'Boston Celtics',\n",
    "    'CHA': 'Charlotte Hornets',\n",
    "    'CHI': 'Chicago Bulls',\n",
    "    'CLE': 'Cleveland Cavaliers',\n",
    "    'DAL': 'Dallas Mavericks',\n",
    "    'DEN': 'Denver Nuggets',\n",
    "    'DET': 'Detroit Pistons',\n",
    "    'GSW': 'Golden State Warriors',\n",
    "    'HOU': 'Houston Rockets',\n",
    "    'IND': 'Indiana Pacers',\n",
    "    'LAC': 'LA Clippers',\n",
    "    'LAL': 'Los Angeles Lakers',\n",
    "    'MEM': 'Memphis Grizzlies',\n",
    "    'MIA': 'Miami Heat',\n",
    "    'MIL': 'Milwaukee Bucks',\n",
    "    'MIN': 'Minnesota Timberwolves',\n",
    "    'NOP': 'New Orleans Pelicans',\n",
    "    'NYK': 'New York Knicks',\n",
    "    'OKC': 'Oklahoma City Thunder',\n",
    "    'ORL': 'Orlando Magic',\n",
    "    'PHI': 'Philadelphia 76ers',\n",
    "    'PHX': 'Phoenix Suns',\n",
    "    'POR': 'Portland Trail Blazers',\n",
    "    'SAC': 'Sacramento Kings',\n",
    "    'SAS': 'San Antonio Spurs',\n",
    "    'TOR': 'Toronto Raptors',\n",
    "    'UTA': 'Utah Jazz',\n",
    "    'WAS': 'Washington Wizards'\n",
    "}\n",
    "\n",
    "# Extract opponent team abbreviation from MATCHUP column\n",
    "def extract_opponent(matchup):\n",
    "    \"\"\"Extract opponent team abbreviation from MATCHUP string.\"\"\"\n",
    "    if ' @ ' in matchup:\n",
    "        return matchup.split(' @ ')[1]\n",
    "    elif ' vs. ' in matchup:\n",
    "        return matchup.split(' vs. ')[1]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_all['OPP_TEAM_ABBREV'] = df_all['MATCHUP'].apply(extract_opponent)\n",
    "df_all['OPP_TEAM_NAME'] = df_all['OPP_TEAM_ABBREV'].map(TEAM_ABBREV_TO_NAME)\n",
    "\n",
    "# Convert SEASON_ID to season format\n",
    "def season_id_to_season(season_id):\n",
    "    \"\"\"Convert SEASON_ID (22019) to season format (2019-20).\"\"\"\n",
    "    year = str(season_id)[1:]\n",
    "    next_year = str(int(year) + 1)[-2:]\n",
    "    return f\"{year}-{next_year}\"\n",
    "\n",
    "df_all['SEASON'] = df_all['SEASON_ID'].apply(season_id_to_season)\n",
    "\n",
    "print(f\"\\nMerging opponent stats into game logs...\")\n",
    "print(f\"   Game logs before merge: {df_all.shape}\")\n",
    "print(f\"   Team stats available: {df_team_stats_all.shape}\")\n",
    "\n",
    "# Select and rename opponent stats\n",
    "df_opponent_stats = df_team_stats_all[['TEAM_NAME', 'SEASON', 'DEF_RATING', 'OFF_RATING', \n",
    "                                         'PACE', 'W', 'L', 'W_PCT']].copy()\n",
    "\n",
    "print(f\"\\n   Sample team names in team stats: {df_opponent_stats['TEAM_NAME'].unique()[:5]}\")\n",
    "print(f\"   Sample OPP_TEAM_NAME in game logs: {df_all['OPP_TEAM_NAME'].unique()[:5]}\")\n",
    "print(f\"   Sample SEASON in team stats: {df_opponent_stats['SEASON'].unique()}\")\n",
    "print(f\"   Sample SEASON in game logs: {df_all['SEASON'].unique()}\")\n",
    "\n",
    "# Merge on opponent team name + season\n",
    "df_all = df_all.merge(\n",
    "    df_opponent_stats,\n",
    "    left_on=['OPP_TEAM_NAME', 'SEASON'],\n",
    "    right_on=['TEAM_NAME', 'SEASON'],\n",
    "    how='left',\n",
    "    suffixes=('', '_OPP')\n",
    ")\n",
    "\n",
    "# Rename the merged columns to have OPP_ prefix\n",
    "df_all = df_all.rename(columns={\n",
    "    'DEF_RATING': 'OPP_DEF_RATING',\n",
    "    'OFF_RATING': 'OPP_OFF_RATING',\n",
    "    'PACE': 'OPP_PACE',\n",
    "    'W': 'OPP_W',\n",
    "    'L': 'OPP_L',\n",
    "    'W_PCT': 'OPP_W_PCT'\n",
    "})\n",
    "\n",
    "# Drop redundant TEAM_NAME column\n",
    "df_all = df_all.drop(columns=['TEAM_NAME'], errors='ignore')\n",
    "\n",
    "print(f\"\\nMerge complete!\")\n",
    "print(f\"   Game logs after merge: {df_all.shape}\")\n",
    "\n",
    "# Check merge success\n",
    "if 'OPP_DEF_RATING' in df_all.columns:\n",
    "    missing = df_all['OPP_DEF_RATING'].isnull().sum()\n",
    "    if missing == 0:\n",
    "        print(f\"   SUCCESS: All {len(df_all):,} games have opponent stats\")\n",
    "    else:\n",
    "        pct = missing / len(df_all) * 100\n",
    "        print(f\"   WARNING: {missing:,} games ({pct:.1f}%) missing opponent stats\")\n",
    "        \n",
    "        # Debug: show unmapped teams\n",
    "        unmapped = df_all[df_all['OPP_DEF_RATING'].isnull()]['OPP_TEAM_NAME'].value_counts()\n",
    "        if len(unmapped) > 0:\n",
    "            print(f\"\\n   Unmapped opponent teams:\")\n",
    "            for team, count in unmapped.items():\n",
    "                print(f\"      {team}: {count} games\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nSample opponent stats:\")\n",
    "    sample = df_all[['MATCHUP', 'OPP_TEAM_ABBREV', 'OPP_DEF_RATING', 'OPP_OFF_RATING', 'OPP_PACE']].head(3)\n",
    "    print(sample.to_string(index=False))\n",
    "else:\n",
    "    print(f\"   ERROR: OPP_DEF_RATING column not created!\")\n",
    "    print(f\"   Columns after merge: {df_all.columns.tolist()}\")\n",
    "\n",
    "print(f\"\\nOpponent-related columns added:\")\n",
    "opp_cols = [col for col in df_all.columns if 'OPP' in col.upper()]\n",
    "print(f\"   {opp_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Cleanup: Remove Duplicate Opponent Columns\n",
      "============================================================\n",
      "\n",
      "No duplicate columns found\n",
      "\n",
      "Final game logs shape: (72509, 37)\n",
      "Final opponent columns:\n",
      "   ['OPP_TEAM_ABBREV', 'OPP_TEAM_NAME', 'OPP_DEF_RATING', 'OPP_OFF_RATING', 'OPP_PACE', 'OPP_W', 'OPP_L', 'OPP_W_PCT']\n",
      "\n",
      "All required opponent stat columns present!\n",
      "\n",
      "Opponent stats summary:\n",
      "       OPP_DEF_RATING  OPP_OFF_RATING      OPP_PACE     OPP_W_PCT\n",
      "count    72509.000000    72509.000000  72509.000000  72509.000000\n",
      "mean       112.407383      112.413351     99.609503      0.499810\n",
      "std          3.117331        3.567888      1.933199      0.141794\n",
      "min        102.500000      102.800000     95.640000      0.171000\n",
      "25%        110.600000      110.100000     98.020000      0.415000\n",
      "50%        112.300000      112.700000     99.410000      0.524000\n",
      "75%        114.400000      114.600000    101.020000      0.610000\n",
      "max        119.600000      122.200000    105.510000      0.780000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Cleanup: Remove Duplicate Opponent Columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Remove duplicate columns with _x and _y suffixes\n",
    "# These were created from running the merge cell multiple times\n",
    "duplicate_suffixes = ['_x', '_y']\n",
    "cols_to_drop = [col for col in df_all.columns if any(col.endswith(suffix) for suffix in duplicate_suffixes)]\n",
    "\n",
    "if cols_to_drop:\n",
    "    print(f\"\\nFound {len(cols_to_drop)} duplicate columns to remove:\")\n",
    "    for col in cols_to_drop:\n",
    "        print(f\"   - {col}\")\n",
    "    \n",
    "    df_all = df_all.drop(columns=cols_to_drop)\n",
    "    print(f\"\\nRemoved duplicate columns\")\n",
    "else:\n",
    "    print(f\"\\nNo duplicate columns found\")\n",
    "\n",
    "# Also drop the old OPP_TEAM column if it exists (we use OPP_TEAM_ABBREV now)\n",
    "if 'OPP_TEAM' in df_all.columns and 'OPP_TEAM_ABBREV' in df_all.columns:\n",
    "    df_all = df_all.drop(columns=['OPP_TEAM'])\n",
    "    print(f\"Removed redundant OPP_TEAM column (keeping OPP_TEAM_ABBREV)\")\n",
    "\n",
    "print(f\"\\nFinal game logs shape: {df_all.shape}\")\n",
    "print(f\"Final opponent columns:\")\n",
    "opp_cols = [col for col in df_all.columns if 'OPP' in col.upper()]\n",
    "print(f\"   {opp_cols}\")\n",
    "\n",
    "# Verify we have the key opponent stats\n",
    "required_opp_cols = ['OPP_DEF_RATING', 'OPP_OFF_RATING', 'OPP_PACE', 'OPP_W_PCT']\n",
    "missing_required = [col for col in required_opp_cols if col not in df_all.columns]\n",
    "\n",
    "if missing_required:\n",
    "    print(f\"\\nWARNING: Missing required columns: {missing_required}\")\n",
    "else:\n",
    "    print(f\"\\nAll required opponent stat columns present!\")\n",
    "    print(f\"\\nOpponent stats summary:\")\n",
    "    print(df_all[required_opp_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Adding Player's Own Team Stats\n",
      "============================================================\n",
      "\n",
      "Merging player's team stats...\n",
      "   Sample player teams: ['POR' 'NOP' 'PHX' 'SAC' 'HOU']\n",
      "\n",
      "Merged player's team stats!\n",
      "   Game logs after merge: (72509, 45)\n",
      "   SUCCESS: All 72,509 games have team stats\n",
      "\n",
      "Sample team vs opponent comparison:\n",
      "    MATCHUP TEAM_ABBREV  TEAM_DEF_RATING  OPP_DEF_RATING  TEAM_OFF_RATING  OPP_OFF_RATING\n",
      "  POR @ BKN         POR            114.3           109.2            113.2           108.7\n",
      "  POR @ DAL         POR            114.3           111.2            113.2           115.9\n",
      "POR vs. PHI         POR            114.3           108.4            113.2           110.7\n",
      "\n",
      "Team-related columns added:\n",
      "   ['OPP_TEAM_ABBREV', 'OPP_TEAM_NAME', 'TEAM_ABBREV', 'TEAM_NAME', 'TEAM_DEF_RATING', 'TEAM_OFF_RATING', 'TEAM_PACE', 'TEAM_W', 'TEAM_L', 'TEAM_W_PCT']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Adding Player's Own Team Stats\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract player's team from MATCHUP column\n",
    "# MATCHUP format: \"POR @ BKN\" (POR is player's team) or \"POR vs. LAC\" (POR is player's team)\n",
    "def extract_own_team(matchup):\n",
    "    \"\"\"Extract player's team abbreviation from MATCHUP string.\"\"\"\n",
    "    if ' @ ' in matchup:\n",
    "        # Away game: \"POR @ BKN\" -> player's team is POR\n",
    "        return matchup.split(' @ ')[0]\n",
    "    elif ' vs. ' in matchup:\n",
    "        # Home game: \"POR vs. LAC\" -> player's team is POR\n",
    "        return matchup.split(' vs. ')[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df_all['TEAM_ABBREV'] = df_all['MATCHUP'].apply(extract_own_team)\n",
    "df_all['TEAM_NAME'] = df_all['TEAM_ABBREV'].map(TEAM_ABBREV_TO_NAME)\n",
    "\n",
    "print(f\"\\nMerging player's team stats...\")\n",
    "print(f\"   Sample player teams: {df_all['TEAM_ABBREV'].unique()[:5]}\")\n",
    "\n",
    "# Select player's team stats\n",
    "df_player_team_stats = df_team_stats_all[['TEAM_NAME', 'SEASON', 'DEF_RATING', 'OFF_RATING',\n",
    "                                           'PACE', 'W', 'L', 'W_PCT']].copy()\n",
    "\n",
    "# Merge on player's team name + season\n",
    "df_all = df_all.merge(\n",
    "    df_player_team_stats,\n",
    "    left_on=['TEAM_NAME', 'SEASON'],\n",
    "    right_on=['TEAM_NAME', 'SEASON'],\n",
    "    how='left',\n",
    "    suffixes=('', '_TEAM')\n",
    ")\n",
    "\n",
    "# Rename columns to have TEAM_ prefix\n",
    "df_all = df_all.rename(columns={\n",
    "    'DEF_RATING': 'TEAM_DEF_RATING',\n",
    "    'OFF_RATING': 'TEAM_OFF_RATING',\n",
    "    'PACE': 'TEAM_PACE',\n",
    "    'W': 'TEAM_W',\n",
    "    'L': 'TEAM_L',\n",
    "    'W_PCT': 'TEAM_W_PCT'\n",
    "})\n",
    "\n",
    "print(f\"\\nMerged player's team stats!\")\n",
    "print(f\"   Game logs after merge: {df_all.shape}\")\n",
    "\n",
    "# Verify merge\n",
    "if 'TEAM_DEF_RATING' in df_all.columns:\n",
    "    missing = df_all['TEAM_DEF_RATING'].isnull().sum()\n",
    "    if missing == 0:\n",
    "        print(f\"   SUCCESS: All {len(df_all):,} games have team stats\")\n",
    "    else:\n",
    "        pct = missing / len(df_all) * 100\n",
    "        print(f\"   WARNING: {missing:,} games ({pct:.1f}%) missing team stats\")\n",
    "        \n",
    "    # Show sample\n",
    "    print(f\"\\nSample team vs opponent comparison:\")\n",
    "    sample_cols = ['MATCHUP', 'TEAM_ABBREV', 'TEAM_DEF_RATING', 'OPP_DEF_RATING', \n",
    "                   'TEAM_OFF_RATING', 'OPP_OFF_RATING']\n",
    "    print(df_all[sample_cols].head(3).to_string(index=False))\n",
    "else:\n",
    "    print(f\"   ERROR: TEAM_DEF_RATING not created!\")\n",
    "\n",
    "print(f\"\\nTeam-related columns added:\")\n",
    "team_cols = [col for col in df_all.columns if 'TEAM_' in col]\n",
    "print(f\"   {team_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Adding Rest Days & Back-to-Back Indicators\n",
      "============================================================\n",
      "\n",
      "Rest days distribution:\n",
      "   Mean days rest: 5.5\n",
      "   Median days rest: 2\n",
      "   First games (DAYS_REST = -1): 289\n",
      "\n",
      "Back-to-back statistics:\n",
      "   Total B2B games: 11,188 (15.4% of all games)\n",
      "\n",
      "Rest days breakdown (excluding first games):\n",
      "   1 day(s) rest: 11,188 games (15.4%)\n",
      "   2 day(s) rest: 42,566 games (58.7%)\n",
      "   3 day(s) rest: 10,303 games (14.2%)\n",
      "   4 day(s) rest: 2,580 games (3.6%)\n",
      "   5 day(s) rest: 988 games (1.4%)\n",
      "   6 day(s) rest: 542 games (0.7%)\n",
      "   7 day(s) rest: 433 games (0.6%)\n",
      "\n",
      "Rest days calculated successfully!\n",
      "   New columns: DAYS_REST, IS_B2B\n",
      "   Note: DAYS_REST = -1 indicates first game for that player in dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Adding Rest Days & Back-to-Back Indicators\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# IMPORTANT: Convert GAME_DATE to datetime first (it's currently a string from API)\n",
    "df_all['GAME_DATE'] = pd.to_datetime(df_all['GAME_DATE'])\n",
    "\n",
    "# Sort by player and date to compute rest days\n",
    "# NOTE: Using 'Player_ID' (mixed case) because standardization happens later in Cell 23\n",
    "df_all = df_all.sort_values(['Player_ID', 'GAME_DATE']).reset_index(drop=True)\n",
    "\n",
    "# Calculate days since last game for each player\n",
    "df_all['DAYS_REST'] = df_all.groupby('Player_ID')['GAME_DATE'].diff().dt.days\n",
    "\n",
    "# For first game of each player in our dataset, there's no previous game to compare\n",
    "# Fill with -1 to indicate \"first game in dataset for this player\"\n",
    "df_all['DAYS_REST'] = df_all['DAYS_REST'].fillna(-1).astype(int)\n",
    "\n",
    "# Flag back-to-back games (games on consecutive days)\n",
    "df_all['IS_B2B'] = (df_all['DAYS_REST'] == 1).astype(int)\n",
    "\n",
    "print(f\"\\nRest days distribution:\")\n",
    "print(f\"   Mean days rest: {df_all[df_all['DAYS_REST'] >= 0]['DAYS_REST'].mean():.1f}\")\n",
    "print(f\"   Median days rest: {df_all[df_all['DAYS_REST'] >= 0]['DAYS_REST'].median():.0f}\")\n",
    "print(f\"   First games (DAYS_REST = -1): {(df_all['DAYS_REST'] == -1).sum():,}\")\n",
    "\n",
    "print(f\"\\nBack-to-back statistics:\")\n",
    "b2b_count = df_all['IS_B2B'].sum()\n",
    "b2b_pct = b2b_count / len(df_all) * 100\n",
    "print(f\"   Total B2B games: {b2b_count:,} ({b2b_pct:.1f}% of all games)\")\n",
    "\n",
    "print(f\"\\nRest days breakdown (excluding first games):\")\n",
    "rest_dist = df_all[df_all['DAYS_REST'] >= 0]['DAYS_REST'].value_counts().sort_index()\n",
    "for days, count in rest_dist.head(7).items():\n",
    "    pct = count / len(df_all) * 100\n",
    "    print(f\"   {days} day(s) rest: {count:,} games ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\nRest days calculated successfully!\")\n",
    "print(f\"   New columns: DAYS_REST, IS_B2B\")\n",
    "print(f\"   Note: DAYS_REST = -1 indicates first game for that player in dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Removing Redundant Columns\n",
      "============================================================\n",
      "Removed SEASON_ID (keeping SEASON)\n",
      "Removed VIDEO_AVAILABLE (not needed for modeling)\n",
      "\n",
      "Column count: 47 -> 45 (2 removed)\n",
      "\n",
      "Final column count by category:\n",
      "   Player info: 2\n",
      "   Game info: 2\n",
      "   Stats: 14\n",
      "   Team context: 10\n",
      "   Opponent context: 8\n",
      "   Rest/schedule: 2\n",
      "\n",
      "Cleanup complete!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Removing Redundant Columns\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Track columns before cleanup\n",
    "cols_before = len(df_all.columns)\n",
    "\n",
    "# Remove redundant columns\n",
    "redundant_cols = []\n",
    "\n",
    "# 1. Remove SEASON_ID (we have SEASON which is more readable)\n",
    "if 'SEASON_ID' in df_all.columns and 'SEASON' in df_all.columns:\n",
    "    df_all = df_all.drop(columns=['SEASON_ID'])\n",
    "    redundant_cols.append('SEASON_ID')\n",
    "    print(f\"Removed SEASON_ID (keeping SEASON)\")\n",
    "\n",
    "# 2. Check for any remaining _x or _y suffixed columns from merges\n",
    "duplicate_suffixes = ['_x', '_y', '_TEAM']\n",
    "cols_to_check = [col for col in df_all.columns if any(col.endswith(suffix) for suffix in duplicate_suffixes)]\n",
    "\n",
    "if cols_to_check:\n",
    "    print(f\"\\nWARNING: Found {len(cols_to_check)} columns with merge suffixes:\")\n",
    "    for col in cols_to_check:\n",
    "        print(f\"   - {col}\")\n",
    "\n",
    "# 3. Drop VIDEO_AVAILABLE if it exists (not useful for prediction)\n",
    "if 'VIDEO_AVAILABLE' in df_all.columns:\n",
    "    df_all = df_all.drop(columns=['VIDEO_AVAILABLE'])\n",
    "    redundant_cols.append('VIDEO_AVAILABLE')\n",
    "    print(f\"Removed VIDEO_AVAILABLE (not needed for modeling)\")\n",
    "\n",
    "cols_after = len(df_all.columns)\n",
    "print(f\"\\nColumn count: {cols_before} -> {cols_after} ({cols_before - cols_after} removed)\")\n",
    "\n",
    "print(f\"\\nFinal column count by category:\")\n",
    "print(f\"   Player info: {len([c for c in df_all.columns if 'PLAYER' in c.upper()])}\")\n",
    "print(f\"   Game info: {len([c for c in df_all.columns if 'GAME' in c.upper()])}\")\n",
    "print(f\"   Stats: {len([c for c in df_all.columns if any(x in c for x in ['PTS', 'REB', 'AST', 'FG', 'FT'])])}\")\n",
    "print(f\"   Team context: {len([c for c in df_all.columns if 'TEAM_' in c])}\")\n",
    "print(f\"   Opponent context: {len([c for c in df_all.columns if 'OPP_' in c])}\")\n",
    "print(f\"   Rest/schedule: {len([c for c in df_all.columns if any(x in c for x in ['DAYS_REST', 'IS_B2B'])])}\")\n",
    "\n",
    "print(f\"\\nCleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: Game Logs\n",
    "\n",
    "Before proceeding, let's validate the game log data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GAME LOG VALIDATION\n",
      "============================================================\n",
      "\n",
      "1. Dataset Shape:\n",
      "   Total games: 72,509\n",
      "   Unique players: 289\n",
      "   Columns: 45\n",
      "\n",
      "2. Games per Season:\n",
      "   2019-20: 12,770 games\n",
      "   2020-21: 14,026 games\n",
      "   2021-22: 16,097 games\n",
      "   2022-23: 15,120 games\n",
      "   2023-24: 14,496 games\n",
      "\n",
      "3. Date Range:\n",
      "   Min: 2019-10-22 00:00:00\n",
      "   Max: 2024-04-14 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# 1. Check data shape and completeness\n",
    "print(\"=\"*60)\n",
    "print(\"GAME LOG VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Dataset Shape:\")\n",
    "print(f\"   Total games: {len(df_all):,}\")\n",
    "# NOTE: Using Player_ID (mixed case) - not renamed to PLAYER_ID until Cell 23\n",
    "print(f\"   Unique players: {df_all['Player_ID'].nunique()}\")\n",
    "print(f\"   Columns: {df_all.shape[1]}\")\n",
    "\n",
    "print(\"\\n2. Games per Season:\")\n",
    "# Use SEASON instead of SEASON_ID (which was dropped in Cell 12)\n",
    "for season in sorted(df_all['SEASON'].unique()):\n",
    "    count = (df_all['SEASON'] == season).sum()\n",
    "    print(f\"   {season}: {count:,} games\")\n",
    "\n",
    "print(\"\\n3. Date Range:\")\n",
    "# GAME_DATE already converted to datetime in Cell 11\n",
    "print(f\"   Min: {df_all['GAME_DATE'].min()}\")\n",
    "print(f\"   Max: {df_all['GAME_DATE'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Missing Values Check:\n",
      "   No missing values in critical columns\n",
      "\n",
      "5. Duplicate Games Check:\n",
      "   No duplicate player-game records found\n"
     ]
    }
   ],
   "source": [
    "# 2. Check for missing values in critical columns\n",
    "print(\"\\n4. Missing Values Check:\")\n",
    "# NOTE: Using Player_ID, Game_ID (mixed case) - not renamed until Cell 23\n",
    "critical_cols = ['Player_ID', 'Game_ID', 'GAME_DATE', 'PTS', 'REB', 'AST', 'MIN', 'FGA', 'FG_PCT']\n",
    "missing = df_all[critical_cols].isnull().sum()\n",
    "\n",
    "if missing.sum() == 0:\n",
    "    print(\"   No missing values in critical columns\")\n",
    "else:\n",
    "    print(\"   WARNING: Missing values found:\")\n",
    "    for col, count in missing[missing > 0].items():\n",
    "        print(f\"      {col}: {count} ({count/len(df_all)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n5. Duplicate Games Check:\")\n",
    "# Using Player_ID, Game_ID (mixed case)\n",
    "duplicates = df_all.duplicated(subset=['Player_ID', 'Game_ID']).sum()\n",
    "if duplicates == 0:\n",
    "    print(\"   No duplicate player-game records found\")\n",
    "else:\n",
    "    print(f\"   WARNING: Found {duplicates} duplicate records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Target Variable Distributions:\n",
      "   PTS: 13.2 +/- 9.2 (range: 0-73)\n",
      "   REB: 4.8 +/- 3.6 (range: 0-31)\n",
      "   AST: 3.0 +/- 2.8 (range: 0-24)\n",
      "\n",
      "7. Data Type Validation:\n",
      "   All data types correct\n",
      "\n",
      "8. Required Columns Check:\n",
      "   All 17 required columns present\n"
     ]
    }
   ],
   "source": [
    "# 3. Validate data distributions\n",
    "print(\"\\n6. Target Variable Distributions:\")\n",
    "print(f\"   PTS: {df_all['PTS'].mean():.1f} +/- {df_all['PTS'].std():.1f} (range: {df_all['PTS'].min()}-{df_all['PTS'].max()})\")\n",
    "print(f\"   REB: {df_all['REB'].mean():.1f} +/- {df_all['REB'].std():.1f} (range: {df_all['REB'].min()}-{df_all['REB'].max()})\")\n",
    "print(f\"   AST: {df_all['AST'].mean():.1f} +/- {df_all['AST'].std():.1f} (range: {df_all['AST'].min()}-{df_all['AST'].max()})\")\n",
    "\n",
    "print(\"\\n7. Data Type Validation:\")\n",
    "# NOTE: Using Player_ID (mixed case) - not renamed until Cell 23\n",
    "expected_types = {\n",
    "    'Player_ID': 'int64',\n",
    "    'PTS': 'int64',\n",
    "    'REB': 'int64',\n",
    "    'AST': 'int64',\n",
    "    'FG_PCT': 'float64'\n",
    "}\n",
    "\n",
    "all_correct = True\n",
    "for col, expected in expected_types.items():\n",
    "    actual = str(df_all[col].dtype)\n",
    "    if actual != expected:\n",
    "        print(f\"   WARNING: {col} - expected {expected}, got {actual}\")\n",
    "        all_correct = False\n",
    "\n",
    "if all_correct:\n",
    "    print(\"   All data types correct\")\n",
    "\n",
    "print(\"\\n8. Required Columns Check:\")\n",
    "# NOTE: Using Player_ID, Game_ID (mixed case) and SEASON (SEASON_ID dropped in Cell 12)\n",
    "required_cols = ['SEASON', 'Player_ID', 'Game_ID', 'GAME_DATE', 'MATCHUP', \n",
    "                 'MIN', 'FGA', 'FG_PCT', 'FG3A', 'FG3_PCT', 'FTA', 'FT_PCT',\n",
    "                 'REB', 'AST', 'PTS', 'TOV', 'PLAYER_NAME']\n",
    "missing_cols = [col for col in required_cols if col not in df_all.columns]\n",
    "\n",
    "if len(missing_cols) == 0:\n",
    "    print(f\"   All {len(required_cols)} required columns present\")\n",
    "else:\n",
    "    print(f\"   WARNING: Missing columns: {missing_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11. Required Shot Columns Check:\n",
      "   All 21 required shot columns present\n",
      "\n",
      "============================================================\n",
      "VALIDATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "Game logs validated: 72,509 games\n",
      "Shot charts validated: 597,106 shots\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n11. Required Shot Columns Check:\")\n",
    "required_shot_cols = ['GRID_TYPE', 'GAME_ID', 'GAME_EVENT_ID', 'PLAYER_ID', \n",
    "                      'PLAYER_NAME', 'TEAM_ID', 'TEAM_NAME', 'PERIOD', \n",
    "                      'MINUTES_REMAINING', 'SECONDS_REMAINING', 'EVENT_TYPE',\n",
    "                      'SHOT_ZONE_BASIC', 'SHOT_ZONE_AREA', 'SHOT_ZONE_RANGE',\n",
    "                      'SHOT_DISTANCE', 'LOC_X', 'LOC_Y', 'SHOT_ATTEMPTED_FLAG',\n",
    "                      'SHOT_MADE_FLAG', 'GAME_DATE', 'Season']\n",
    "\n",
    "missing_shot_cols = [col for col in required_shot_cols if col not in df_shots.columns]\n",
    "\n",
    "if len(missing_shot_cols) == 0:\n",
    "    print(f\"   All {len(required_shot_cols)} required shot columns present\")\n",
    "else:\n",
    "    print(f\"   WARNING: Missing columns: {missing_shot_cols}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"VALIDATION COMPLETE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nGame logs validated: {len(df_all):,} games\")\n",
    "print(f\"Shot charts validated: {len(df_shots):,} shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "8. Shot Zone Distribution:\n",
      "   Total zones: 7\n",
      "   Above the Break 3: 180,949 (30.3%)\n",
      "   Restricted Area: 169,853 (28.4%)\n",
      "   In The Paint (Non-RA): 109,020 (18.3%)\n",
      "   Mid-Range: 82,185 (13.8%)\n",
      "   Left Corner 3: 28,279 (4.7%)\n",
      "   Right Corner 3: 25,697 (4.3%)\n",
      "   Backcourt: 1,123 (0.2%)\n",
      "\n",
      "9. Shot Made/Attempted Validation:\n",
      "   Total attempts: 597,106\n",
      "   Total makes: 278,752\n",
      "   Overall FG%: 46.7%\n",
      "   FG% in expected NBA range (40-50%)\n",
      "\n",
      "10. Shot Distance Distribution:\n",
      "   Min: 0 ft\n",
      "   Max: 87 ft\n",
      "   Mean: 13.9 ft\n",
      "   Median: 14.0 ft\n",
      "   All shot distances within court dimensions\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n8. Shot Zone Distribution:\")\n",
    "zone_counts = df_shots['SHOT_ZONE_BASIC'].value_counts()\n",
    "print(f\"   Total zones: {len(zone_counts)}\")\n",
    "for zone, count in zone_counts.items():\n",
    "    pct = count / len(df_shots) * 100\n",
    "    print(f\"   {zone}: {count:,} ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"\\n9. Shot Made/Attempted Validation:\")\n",
    "total_attempted = df_shots['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "total_made = df_shots['SHOT_MADE_FLAG'].sum()\n",
    "overall_fg_pct = (total_made / total_attempted * 100) if total_attempted > 0 else 0\n",
    "\n",
    "print(f\"   Total attempts: {total_attempted:,}\")\n",
    "print(f\"   Total makes: {total_made:,}\")\n",
    "print(f\"   Overall FG%: {overall_fg_pct:.1f}%\")\n",
    "\n",
    "# Sanity check: NBA league average is typically 45-47%\n",
    "if 40 <= overall_fg_pct <= 50:\n",
    "    print(f\"   FG% in expected NBA range (40-50%)\")\n",
    "else:\n",
    "    print(f\"   WARNING: FG% outside typical NBA range - check data quality\")\n",
    "\n",
    "print(f\"\\n10. Shot Distance Distribution:\")\n",
    "print(f\"   Min: {df_shots['SHOT_DISTANCE'].min()} ft\")\n",
    "print(f\"   Max: {df_shots['SHOT_DISTANCE'].max()} ft\")\n",
    "print(f\"   Mean: {df_shots['SHOT_DISTANCE'].mean():.1f} ft\")\n",
    "print(f\"   Median: {df_shots['SHOT_DISTANCE'].median():.1f} ft\")\n",
    "\n",
    "# Sanity check: NBA court is 94 ft long, max shot distance ~90 ft\n",
    "if df_shots['SHOT_DISTANCE'].max() <= 95:\n",
    "    print(f\"   All shot distances within court dimensions\")\n",
    "else:\n",
    "    print(f\"   WARNING: Some shots beyond court dimensions - check data quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Missing Values Check:\n",
      "   No missing values in critical shot columns\n",
      "\n",
      "6. Duplicate Shots Check:\n",
      "   WARNING: Found 445 potential duplicate shots\n",
      "      (0.075% of total - likely edge cases)\n",
      "\n",
      "7. Orphaned Shots Check:\n",
      "   Checking if all shots have corresponding game logs...\n",
      "   WARNING: 446 player-game pairs in shots but not in game logs\n",
      "      (0.8% of player-game pairs)\n",
      "      Note: Orphans are typically from players with < 1 minute played\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n5. Missing Values Check:\")\n",
    "shot_critical_cols = ['PLAYER_ID', 'GAME_ID', 'GAME_DATE', 'SHOT_ZONE_BASIC', \n",
    "                      'SHOT_DISTANCE', 'SHOT_MADE_FLAG', 'SHOT_ATTEMPTED_FLAG']\n",
    "missing_shots = df_shots[shot_critical_cols].isnull().sum()\n",
    "\n",
    "if missing_shots.sum() == 0:\n",
    "    print(\"   No missing values in critical shot columns\")\n",
    "else:\n",
    "    print(\"   WARNING: Missing values found:\")\n",
    "    for col, count in missing_shots[missing_shots > 0].items():\n",
    "        print(f\"      {col}: {count} ({count/len(df_shots)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n6. Duplicate Shots Check:\")\n",
    "shot_duplicates = df_shots.duplicated(subset=['PLAYER_ID', 'GAME_ID', 'PERIOD', \n",
    "                                               'MINUTES_REMAINING', 'SECONDS_REMAINING']).sum()\n",
    "if shot_duplicates == 0:\n",
    "    print(\"   No duplicate shot records found\")\n",
    "else:\n",
    "    print(f\"   WARNING: Found {shot_duplicates} potential duplicate shots\")\n",
    "    pct = shot_duplicates / len(df_shots) * 100\n",
    "    print(f\"      ({pct:.3f}% of total - likely edge cases)\")\n",
    "\n",
    "print(\"\\n7. Orphaned Shots Check:\")\n",
    "print(\"   Checking if all shots have corresponding game logs...\")\n",
    "# NOTE: df_all still has Player_ID, Game_ID (mixed case) at this point - not renamed until Cell 23\n",
    "# df_shots has PLAYER_ID, GAME_ID (all caps) from NBA API\n",
    "valid_player_game_pairs = set(zip(df_all['Player_ID'], df_all['Game_ID']))\n",
    "shot_player_game_pairs = set(zip(df_shots['PLAYER_ID'], df_shots['GAME_ID']))\n",
    "orphaned = len(shot_player_game_pairs - valid_player_game_pairs)\n",
    "\n",
    "if orphaned == 0:\n",
    "    print(\"   All shots have corresponding game logs\")\n",
    "else:\n",
    "    print(f\"   WARNING: {orphaned} player-game pairs in shots but not in game logs\")\n",
    "    pct = orphaned / len(shot_player_game_pairs) * 100\n",
    "    print(f\"      ({pct:.1f}% of player-game pairs)\")\n",
    "    print(\"      Note: Orphans are typically from players with < 1 minute played\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SHOT CHART VALIDATION\n",
      "============================================================\n",
      "\n",
      "1. Dataset Shape:\n",
      "   Total shots: 597,106\n",
      "   Unique players: 229\n",
      "   Unique games: 5829\n",
      "   Columns: 25\n",
      "\n",
      "2. Shots per Season:\n",
      "   2019-20: 111,955 shots\n",
      "   2020-21: 106,492 shots\n",
      "   2021-22: 130,284 shots\n",
      "   2022-23: 124,566 shots\n",
      "   2023-24: 123,809 shots\n",
      "\n",
      "3. Date Range:\n",
      "   Min: 2019-10-22 00:00:00\n",
      "   Max: 2024-04-14 00:00:00\n",
      "\n",
      "4. Date Alignment Check:\n",
      "   Shot dates align with game log dates\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SHOT CHART VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n1. Dataset Shape:\")\n",
    "print(f\"   Total shots: {len(df_shots):,}\")\n",
    "# NOTE: Shots have PLAYER_ID (all caps) from NBA API\n",
    "print(f\"   Unique players: {df_shots['PLAYER_ID'].nunique()}\")\n",
    "print(f\"   Unique games: {df_shots['GAME_ID'].nunique()}\")\n",
    "print(f\"   Columns: {df_shots.shape[1]}\")\n",
    "\n",
    "print(\"\\n2. Shots per Season:\")\n",
    "for season in SEASONS:\n",
    "    count = (df_shots['Season'] == season).sum()\n",
    "    print(f\"   {season}: {count:,} shots\")\n",
    "\n",
    "print(\"\\n3. Date Range:\")\n",
    "df_shots['GAME_DATE'] = pd.to_datetime(df_shots['GAME_DATE'])\n",
    "print(f\"   Min: {df_shots['GAME_DATE'].min()}\")\n",
    "print(f\"   Max: {df_shots['GAME_DATE'].max()}\")\n",
    "\n",
    "print(\"\\n4. Date Alignment Check:\")\n",
    "shots_date_range = (df_shots['GAME_DATE'].min(), df_shots['GAME_DATE'].max())\n",
    "games_date_range = (df_all['GAME_DATE'].min(), df_all['GAME_DATE'].max())\n",
    "if shots_date_range[0] >= games_date_range[0] and shots_date_range[1] <= games_date_range[1]:\n",
    "    print(\"   Shot dates align with game log dates\")\n",
    "else:\n",
    "    print(f\"   WARNING: Date mismatch\")\n",
    "    print(f\"   Shot dates: {shots_date_range}\")\n",
    "    print(f\"   Game dates: {games_date_range}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now that we've validated the raw data, let's clean it up for feature engineering:\n",
    "1. **Fix duplicate columns** in shot charts (PLAYER_ID vs Player_ID)\n",
    "2. **Standardize column naming** across both datasets\n",
    "3. **Remove duplicate shot records** (574 found)\n",
    "4. **Filter orphaned shots** without corresponding game logs (664 player-game pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: Fix Duplicate Columns in Shot Charts\n",
      "============================================================\n",
      "\n",
      "Current shot chart columns (25 total):\n",
      "   Player-related columns: ['PLAYER_ID', 'PLAYER_NAME']\n",
      "\n",
      "After cleanup: 25 columns remaining\n",
      "   Player columns: ['PLAYER_ID', 'PLAYER_NAME']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"STEP 1: Fix Duplicate Columns in Shot Charts\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check current columns\n",
    "print(f\"\\nCurrent shot chart columns ({len(df_shots.columns)} total):\")\n",
    "player_cols = [col for col in df_shots.columns if 'PLAYER' in col.upper()]\n",
    "print(f\"   Player-related columns: {player_cols}\")\n",
    "\n",
    "# The NBA API already provides PLAYER_ID and PLAYER_NAME\n",
    "# We mistakenly added Player_ID and Player_Name duplicates in cell 5\n",
    "# Let's remove the manually added ones and keep the API originals\n",
    "\n",
    "if 'PLAYER_ID' in df_shots.columns and 'Player_ID' in df_shots.columns:\n",
    "    # Verify they're actually duplicates\n",
    "    id_mismatch = (df_shots['PLAYER_ID'] != df_shots['Player_ID']).sum()\n",
    "    if id_mismatch == 0:\n",
    "        print(f\"\\nVerified: PLAYER_ID and Player_ID are identical (0 mismatches)\")\n",
    "        df_shots = df_shots.drop(columns=['Player_ID'])\n",
    "        print(f\"   Dropped duplicate 'Player_ID' column\")\n",
    "    else:\n",
    "        print(f\"\\nWARNING: Found {id_mismatch} mismatches between PLAYER_ID and Player_ID\")\n",
    "\n",
    "if 'PLAYER_NAME' in df_shots.columns and 'Player_Name' in df_shots.columns:\n",
    "    name_mismatch = (df_shots['PLAYER_NAME'] != df_shots['Player_Name']).sum()\n",
    "    if name_mismatch == 0:\n",
    "        print(f\"Verified: PLAYER_NAME and Player_Name are identical (0 mismatches)\")\n",
    "        df_shots = df_shots.drop(columns=['Player_Name'])\n",
    "        print(f\"   Dropped duplicate 'Player_Name' column\")\n",
    "    else:\n",
    "        print(f\"\\nWARNING: Found {name_mismatch} mismatches between PLAYER_NAME and Player_Name\")\n",
    "\n",
    "print(f\"\\nAfter cleanup: {len(df_shots.columns)} columns remaining\")\n",
    "print(f\"   Player columns: {[col for col in df_shots.columns if 'PLAYER' in col.upper()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: Standardize Column Naming\n",
      "============================================================\n",
      "\n",
      "Before standardization:\n",
      "   Game logs: Player_ID, Game_ID\n",
      "   Shot charts: PLAYER_ID, GAME_ID\n",
      "\n",
      "Renamed 2 columns in game logs:\n",
      "   Player_ID -> PLAYER_ID\n",
      "   Game_ID -> GAME_ID\n",
      "\n",
      "After standardization:\n",
      "   Game logs: ['PLAYER_ID', 'GAME_ID', 'GAME_DATE', 'PLAYER_NAME']\n",
      "   Shot charts: ['GAME_ID', 'GAME_EVENT_ID', 'PLAYER_ID', 'PLAYER_NAME', 'GAME_DATE']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: Standardize Column Naming\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Game logs have Player_ID and Game_ID (mixed case from NBA API)\n",
    "# Shot charts have PLAYER_ID (all caps from NBA API)\n",
    "# Let's standardize everything to UPPERCASE to match NBA API convention\n",
    "\n",
    "print(\"\\nBefore standardization:\")\n",
    "print(f\"   Game logs: Player_ID, Game_ID\")\n",
    "print(f\"   Shot charts: PLAYER_ID, GAME_ID\")\n",
    "\n",
    "# Rename game log columns to uppercase\n",
    "rename_mapping = {\n",
    "    'Player_ID': 'PLAYER_ID',\n",
    "    'Game_ID': 'GAME_ID'\n",
    "}\n",
    "\n",
    "# Only rename if the mixed-case version exists\n",
    "cols_to_rename = {k: v for k, v in rename_mapping.items() if k in df_all.columns}\n",
    "if cols_to_rename:\n",
    "    df_all = df_all.rename(columns=cols_to_rename)\n",
    "    print(f\"\\nRenamed {len(cols_to_rename)} columns in game logs:\")\n",
    "    for old, new in cols_to_rename.items():\n",
    "        print(f\"   {old} -> {new}\")\n",
    "else:\n",
    "    print(f\"\\nGame logs already use standard naming\")\n",
    "\n",
    "print(f\"\\nAfter standardization:\")\n",
    "print(f\"   Game logs: {[col for col in df_all.columns if 'PLAYER' in col.upper() or 'GAME' in col.upper()][:5]}\")\n",
    "print(f\"   Shot charts: {[col for col in df_shots.columns if 'PLAYER' in col.upper() or 'GAME' in col.upper()][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: Remove Duplicate Shots\n",
      "============================================================\n",
      "\n",
      "Before deduplication: 597,106 shots\n",
      "\n",
      "Removed 445 duplicate shot records\n",
      "   (0.075% of original data)\n",
      "\n",
      "After deduplication: 596,661 shots\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 3: Remove Duplicate Shots\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nBefore deduplication: {len(df_shots):,} shots\")\n",
    "\n",
    "# Remove duplicates based on unique shot identifiers\n",
    "# A shot is uniquely identified by: Player, Game, Period, and exact time\n",
    "before_count = len(df_shots)\n",
    "df_shots = df_shots.drop_duplicates(subset=['PLAYER_ID', 'GAME_ID', 'PERIOD', \n",
    "                                              'MINUTES_REMAINING', 'SECONDS_REMAINING'])\n",
    "after_count = len(df_shots)\n",
    "removed = before_count - after_count\n",
    "\n",
    "if removed > 0:\n",
    "    print(f\"\\nRemoved {removed:,} duplicate shot records\")\n",
    "    pct = removed / before_count * 100\n",
    "    print(f\"   ({pct:.3f}% of original data)\")\n",
    "else:\n",
    "    print(f\"\\nNo duplicates found\")\n",
    "\n",
    "print(f\"\\nAfter deduplication: {len(df_shots):,} shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: Filter Orphaned Shots\n",
      "============================================================\n",
      "\n",
      "Before filtering: 596,661 shots\n",
      "\n",
      "Removed 5,194 orphaned shots\n",
      "   (0.871% of shots without corresponding game logs)\n",
      "\n",
      "After filtering: 591,467 shots\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 4: Filter Orphaned Shots\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Orphaned shots = shots without corresponding game logs\n",
    "# These occur when players had shots but the game log API didn't return that game\n",
    "# (typically players with <1 min played)\n",
    "\n",
    "print(f\"\\nBefore filtering: {len(df_shots):,} shots\")\n",
    "\n",
    "# Create set of valid player-game pairs from game logs\n",
    "valid_pairs = set(zip(df_all['PLAYER_ID'], df_all['GAME_ID']))\n",
    "\n",
    "# Filter shots to only keep those with corresponding game logs\n",
    "before_count = len(df_shots)\n",
    "df_shots = df_shots[df_shots.apply(lambda x: (x['PLAYER_ID'], x['GAME_ID']) in valid_pairs, axis=1)]\n",
    "after_count = len(df_shots)\n",
    "removed = before_count - after_count\n",
    "\n",
    "if removed > 0:\n",
    "    print(f\"\\nRemoved {removed:,} orphaned shots\")\n",
    "    pct = removed / before_count * 100\n",
    "    print(f\"   ({pct:.3f}% of shots without corresponding game logs)\")\n",
    "else:\n",
    "    print(f\"\\nNo orphaned shots found\")\n",
    "\n",
    "print(f\"\\nAfter filtering: {len(df_shots):,} shots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 5: Save Cleaned & Enriched Data to Processed Directory\n",
      "============================================================\n",
      "\n",
      "Saving to data/processed/ directory...\n",
      "   (Original raw API data remains in data/raw/)\n",
      "\n",
      "Saved cleaned game logs:\n",
      "   File: data/processed/gamelogs_combined.parquet\n",
      "   Rows: 72,509\n",
      "   Columns: 45\n",
      "\n",
      "Per-season breakdowns:\n",
      "   2019-20: 12,770 games\n",
      "   2020-21: 14,026 games\n",
      "   2021-22: 16,097 games\n",
      "   2022-23: 15,120 games\n",
      "   2023-24: 14,496 games\n",
      "\n",
      "Saved cleaned shot charts:\n",
      "   File: data/processed/shot_charts_all.parquet\n",
      "   Rows: 591,467\n",
      "   Columns: 25\n",
      "\n",
      "============================================================\n",
      "DATA PROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Final Clean Datasets:\n",
      "   Game logs: 72,509 games from 289 players\n",
      "   Shot charts: 591,467 shots from 229 players\n",
      "   Unique player-game pairs in shots: 55,949\n",
      "\n",
      "What's been added to the data:\n",
      "   - Opponent stats (8 columns)\n",
      "   - Player's team stats (6 columns)\n",
      "   - Rest days & back-to-back indicators (2 columns)\n",
      "   - Removed duplicates and orphaned records\n",
      "   - Standardized column naming\n",
      "\n",
      "Next step: Feature engineering (rolling averages, lagged features, etc.)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 5: Save Cleaned & Enriched Data to Processed Directory\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save cleaned datasets to data/processed/ (not raw/ - this data has been significantly modified)\n",
    "print(f\"\\nSaving to data/processed/ directory...\")\n",
    "print(f\"   (Original raw API data remains in data/raw/)\")\n",
    "\n",
    "# Save cleaned game logs\n",
    "df_all.to_parquet(processed_path / \"gamelogs_combined.parquet\", index=False)\n",
    "print(f\"\\nSaved cleaned game logs:\")\n",
    "print(f\"   File: data/processed/gamelogs_combined.parquet\")\n",
    "print(f\"   Rows: {len(df_all):,}\")\n",
    "print(f\"   Columns: {df_all.shape[1]}\")\n",
    "\n",
    "# Save per-season files with cleaned data\n",
    "# Use SEASON column (not SEASON_ID which was dropped in Cell 12)\n",
    "print(f\"\\nPer-season breakdowns:\")\n",
    "for season in SEASONS:\n",
    "    season_data = df_all[df_all['SEASON'] == season]\n",
    "    if len(season_data) > 0:\n",
    "        season_data.to_parquet(processed_path / f\"gamelogs_{season}.parquet\", index=False)\n",
    "        print(f\"   {season}: {len(season_data):,} games\")\n",
    "\n",
    "# Save cleaned shot charts\n",
    "df_shots.to_parquet(processed_path / \"shot_charts_all.parquet\", index=False)\n",
    "print(f\"\\nSaved cleaned shot charts:\")\n",
    "print(f\"   File: data/processed/shot_charts_all.parquet\")\n",
    "print(f\"   Rows: {len(df_shots):,}\")\n",
    "print(f\"   Columns: {df_shots.shape[1]}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"DATA PROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nFinal Clean Datasets:\")\n",
    "print(f\"   Game logs: {len(df_all):,} games from {df_all['PLAYER_ID'].nunique()} players\")\n",
    "print(f\"   Shot charts: {len(df_shots):,} shots from {df_shots['PLAYER_ID'].nunique()} players\")\n",
    "print(f\"   Unique player-game pairs in shots: {len(set(zip(df_shots['PLAYER_ID'], df_shots['GAME_ID']))):,}\")\n",
    "\n",
    "print(f\"\\nWhat's been added to the data:\")\n",
    "print(f\"   - Opponent stats (8 columns)\")\n",
    "print(f\"   - Player's team stats (6 columns)\")\n",
    "print(f\"   - Rest days & back-to-back indicators (2 columns)\")\n",
    "print(f\"   - Removed duplicates and orphaned records\")\n",
    "print(f\"   - Standardized column naming\")\n",
    "\n",
    "print(f\"\\nNext step: Feature engineering (rolling averages, lagged features, etc.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verified: Processed file has 45 columns and 72,509 rows\n",
      "\n",
      "Opponent context columns:\n",
      "   - OPP_TEAM_ABBREV\n",
      "   - OPP_TEAM_NAME\n",
      "   - OPP_DEF_RATING\n",
      "   - OPP_OFF_RATING\n",
      "   - OPP_PACE\n",
      "   - OPP_W\n",
      "   - OPP_L\n",
      "   - OPP_W_PCT\n",
      "\n",
      "Team context columns:\n",
      "   - OPP_TEAM_ABBREV\n",
      "   - OPP_TEAM_NAME\n",
      "   - TEAM_ABBREV\n",
      "   - TEAM_NAME\n",
      "   - TEAM_DEF_RATING\n",
      "   - TEAM_OFF_RATING\n",
      "   - TEAM_PACE\n",
      "   - TEAM_W\n",
      "   - TEAM_L\n",
      "   - TEAM_W_PCT\n",
      "\n",
      "Rest/schedule columns:\n",
      "   - DAYS_REST\n",
      "   - IS_B2B\n",
      "\n",
      "Sample data (sorted by player and date):\n",
      " PLAYER_NAME  GAME_DATE     MATCHUP  PTS  OPP_DEF_RATING  TEAM_DEF_RATING  DAYS_REST  IS_B2B\n",
      "LeBron James 2019-10-22   LAL @ LAC   18           106.9            106.1         -1       0\n",
      "LeBron James 2019-10-25 LAL vs. UTA   32           109.3            106.1          3       0\n",
      "LeBron James 2019-10-27 LAL vs. CHA   20           112.8            106.1          2       0\n",
      "LeBron James 2019-10-29 LAL vs. MEM   23           109.7            106.1          2       0\n",
      "LeBron James 2019-11-01   LAL @ DAL   39           111.2            106.1          3       0\n",
      "\n",
      "Note: DAYS_REST = -1 indicates first game for that player in our dataset\n",
      "\n",
      "Data ready for feature engineering in next notebook!\n"
     ]
    }
   ],
   "source": [
    "# Verify the processed data was saved correctly\n",
    "df_verify = pd.read_parquet(processed_path / \"gamelogs_combined.parquet\")\n",
    "print(f\"Verified: Processed file has {df_verify.shape[1]} columns and {len(df_verify):,} rows\")\n",
    "\n",
    "print(f\"\\nOpponent context columns:\")\n",
    "opp_cols = [c for c in df_verify.columns if 'OPP_' in c]\n",
    "for col in opp_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\nTeam context columns:\")\n",
    "team_cols = [c for c in df_verify.columns if 'TEAM_' in c]\n",
    "for col in team_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "print(f\"\\nRest/schedule columns:\")\n",
    "rest_cols = [c for c in df_verify.columns if any(x in c for x in ['DAYS_REST', 'IS_B2B'])]\n",
    "for col in rest_cols:\n",
    "    print(f\"   - {col}\")\n",
    "\n",
    "# Show a chronologically sorted sample for clarity\n",
    "print(f\"\\nSample data (sorted by player and date):\")\n",
    "sample_cols = ['PLAYER_NAME', 'GAME_DATE', 'MATCHUP', 'PTS', 'OPP_DEF_RATING', 'TEAM_DEF_RATING', 'DAYS_REST', 'IS_B2B']\n",
    "sample_df = df_verify.sort_values(['PLAYER_ID', 'GAME_DATE']).head(5)\n",
    "print(sample_df[sample_cols].to_string(index=False))\n",
    "\n",
    "print(f\"\\nNote: DAYS_REST = -1 indicates first game for that player in our dataset\")\n",
    "print(f\"\\nData ready for feature engineering in next notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Dataset Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINAL DATASET VERIFICATION\n",
      "============================================================\n",
      "\n",
      "Final Processed Dataset:\n",
      "   Location: data/processed/gamelogs_combined.parquet\n",
      "   Rows: 72,509\n",
      "   Columns: 45\n",
      "\n",
      "Enrichments successfully added:\n",
      "   Opponent stats: 8 columns\n",
      "   Team stats: 10 columns\n",
      "   Schedule features: 2 columns\n",
      "\n",
      "Sample data (first 3 games for first player):\n",
      " PLAYER_NAME  GAME_DATE     MATCHUP  PTS  DAYS_REST  OPP_DEF_RATING\n",
      "LeBron James 2019-10-22   LAL @ LAC   18         -1           106.9\n",
      "LeBron James 2019-10-25 LAL vs. UTA   32          3           109.3\n",
      "LeBron James 2019-10-27 LAL vs. CHA   20          2           112.8\n",
      "\n",
      "Data collection and processing complete!\n",
      "Ready for notebook 02: Feature Engineering\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINAL DATASET VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reload from processed directory to verify\n",
    "df_test = pd.read_parquet(processed_path / \"gamelogs_combined.parquet\")\n",
    "\n",
    "print(f\"\\nFinal Processed Dataset:\")\n",
    "print(f\"   Location: data/processed/gamelogs_combined.parquet\")\n",
    "print(f\"   Rows: {len(df_test):,}\")\n",
    "print(f\"   Columns: {df_test.shape[1]}\")\n",
    "\n",
    "print(f\"\\nEnrichments successfully added:\")\n",
    "print(f\"   Opponent stats: {len([c for c in df_test.columns if 'OPP_' in c])} columns\")\n",
    "print(f\"   Team stats: {len([c for c in df_test.columns if 'TEAM_' in c])} columns\")\n",
    "print(f\"   Schedule features: {len([c for c in df_test.columns if any(x in c for x in ['DAYS_REST', 'IS_B2B'])])} columns\")\n",
    "\n",
    "# Show chronologically sorted sample\n",
    "print(f\"\\nSample data (first 3 games for first player):\")\n",
    "sample = df_test.sort_values(['PLAYER_ID', 'GAME_DATE']).head(3)\n",
    "print(sample[['PLAYER_NAME', 'GAME_DATE', 'MATCHUP', 'PTS', 'DAYS_REST', 'OPP_DEF_RATING']].to_string(index=False))\n",
    "\n",
    "print(f\"\\nData collection and processing complete!\")\n",
    "print(f\"Ready for notebook 02: Feature Engineering\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
