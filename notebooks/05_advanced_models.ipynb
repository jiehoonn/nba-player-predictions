{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 05: Advanced Models (Tree Ensembles)\n",
    "\n",
    "## Objective\n",
    "Train advanced tree-based models to improve upon Ridge Regression baseline from Notebook 04.\n",
    "\n",
    "## Why Tree Models (Not Neural Networks)?\n",
    "\n",
    "**Research shows tree ensembles consistently outperform neural networks on tabular data:**\n",
    "- Dataset size (46k samples) is below NN minimum (100k+) for effective training\n",
    "- XGBoost/Random Forest dominate Kaggle competitions on structured data\n",
    "- We've engineered temporal features (rolling avgs) - NNs would need to rediscover these patterns\n",
    "- Better interpretability (feature importance, SHAP values)\n",
    "- Faster training and tuning\n",
    "\n",
    "**Academic justification:**\n",
    "> \"We did not explore neural networks because extensive research demonstrates that tree-based\n",
    "> ensemble methods (Random Forest, XGBoost) consistently outperform deep learning on structured\n",
    "> tabular data, particularly with datasets under 100,000 samples (Chen & Guestrin 2016;\n",
    "> Prokhorenkova et al. 2018).  Given our dataset size (46,824 training samples) and the\n",
    "> performance ceiling imposed by missing game-time features, we focused on proven methods.\"\n",
    "\n",
    "## Strategy\n",
    "1. Load 38-feature dataset from Notebook 03 (train/val/test splits)\n",
    "2. Load Ridge baseline from Notebook 04 (PTS: 5.081, REB: 1.951, AST: 1.491)\n",
    "3. **Random Forest** - Capture non-linear patterns and feature interactions\n",
    "4. **XGBoost** - State-of-the-art gradient boosting for tabular data\n",
    "5. **Ensemble** - Combine Ridge + Random Forest + XGBoost\n",
    "6. **Feature Importance** - Compare what each model learns\n",
    "7. **Error Analysis** - Identify which players/games are hardest to predict\n",
    "8. Evaluate on validation set (RESERVE test set for Notebook 06)\n",
    "\n",
    "## Baseline Performance (from Notebook 04)\n",
    "- **PTS:** Ridge Œ±=10.0 ‚Üí MAE = 5.081, R¬≤ = 0.530\n",
    "- **REB:** Ridge Œ±=1.0 ‚Üí MAE = 1.951, R¬≤ = 0.475\n",
    "- **AST:** Ridge Œ±=100.0 ‚Üí MAE = 1.491, R¬≤ = 0.529\n",
    "\n",
    "## Expected Performance\n",
    "- **Random Forest:** +1-3% over Ridge (MAE: 4.95-5.0 PTS)\n",
    "- **XGBoost:** +2-5% over Ridge (MAE: 4.85-5.0 PTS)\n",
    "- **Ensemble:** +3-6% over Ridge (MAE: 4.8-4.95 PTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports loaded\n",
      "   pandas: 2.3.3\n",
      "   numpy: 2.3.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")\n",
    "print(f\"   pandas: {pd.__version__}\")\n",
    "print(f\"   numpy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data from Notebook 03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train/val splits from Notebook 03...\n",
      "\n",
      "‚úÖ Data loaded\n",
      "\n",
      "üìä Dataset splits:\n",
      "   Train: 46,824 games | 2019-10-28 to 2022-12-31\n",
      "   Val:   13,337 games | 2023-01-01 to 2023-12-31\n",
      "   Test:  8,604 games | 2024-01-01 to 2024-04-14 (RESERVED)\n",
      "\n",
      "üìã Features: 38\n",
      "   Targets: ['PTS', 'REB', 'AST']\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading train/val splits from Notebook 03...\\n\")\n",
    "\n",
    "# Load splits\n",
    "train = pd.read_parquet('../data/processed/train.parquet')\n",
    "val = pd.read_parquet('../data/processed/val.parquet')\n",
    "test = pd.read_parquet('../data/processed/test.parquet')  # DON'T use yet!\n",
    "\n",
    "# Load metadata\n",
    "with open('../data/processed/feature_metadata_v2.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "feature_names = metadata['feature_names']\n",
    "target_cols = metadata['target_columns']\n",
    "\n",
    "print(\"‚úÖ Data loaded\")\n",
    "print(f\"\\nüìä Dataset splits:\")\n",
    "print(f\"   Train: {len(train):,} games | {train['GAME_DATE'].min().date()} to {train['GAME_DATE'].max().date()}\")\n",
    "print(f\"   Val:   {len(val):,} games | {val['GAME_DATE'].min().date()} to {val['GAME_DATE'].max().date()}\")\n",
    "print(f\"   Test:  {len(test):,} games | {test['GAME_DATE'].min().date()} to {test['GAME_DATE'].max().date()} (RESERVED)\")\n",
    "print(f\"\\nüìã Features: {len(feature_names)}\")\n",
    "print(f\"   Targets: {target_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare Features & Load Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing X (features) and y (targets)...\n",
      "\n",
      "‚úÖ Data prepared\n",
      "   X_train: (46824, 38)\n",
      "   X_val:   (13337, 38)\n",
      "\n",
      "======================================================================\n",
      "BASELINE PERFORMANCE (Ridge Regression from Notebook 04)\n",
      "======================================================================\n",
      "\n",
      "   PTS: Ridge (Œ±=10.0)\n",
      "      Val MAE = 5.081, R¬≤ = 0.530\n",
      "\n",
      "   REB: Ridge (Œ±=1.0)\n",
      "      Val MAE = 1.951, R¬≤ = 0.475\n",
      "\n",
      "   AST: Ridge (Œ±=100.0)\n",
      "      Val MAE = 1.491, R¬≤ = 0.529\n",
      "\n",
      "üí° Goal: Beat Ridge by 2-5%\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing X (features) and y (targets)...\\n\")\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = train[feature_names].copy()\n",
    "y_train = train[target_cols].copy()\n",
    "\n",
    "X_val = val[feature_names].copy()\n",
    "y_val = val[target_cols].copy()\n",
    "\n",
    "print(\"‚úÖ Data prepared\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   X_val:   {X_val.shape}\")\n",
    "\n",
    "# Load baseline results from Notebook 04\n",
    "with open('../results/baseline_models_results.json', 'r') as f:\n",
    "    baseline_results = json.load(f)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BASELINE PERFORMANCE (Ridge Regression from Notebook 04)\")\n",
    "print(\"=\" * 70)\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    mae = baseline_results['best_models'][target]['val_mae']\n",
    "    r2 = baseline_results['best_models'][target]['val_r2']\n",
    "    model = baseline_results['best_models'][target]['model']\n",
    "    print(f\"\\n   {target}: {model}\")\n",
    "    print(f\"      Val MAE = {mae:.3f}, R¬≤ = {r2:.3f}\")\n",
    "\n",
    "print(\"\\nüí° Goal: Beat Ridge by 2-5%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on train and validation sets.\n",
    "    \n",
    "    Returns:\n",
    "        dict with metrics and predictions\n",
    "    \"\"\"\n",
    "    # Predict\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'train_mae': mean_absolute_error(y_train, y_pred_train),\n",
    "        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
    "        'train_r2': r2_score(y_train, y_pred_train),\n",
    "        'val_mae': mean_absolute_error(y_val, y_pred_val),\n",
    "        'val_rmse': np.sqrt(mean_squared_error(y_val, y_pred_val)),\n",
    "        'val_r2': r2_score(y_val, y_pred_val)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_pred_train, y_pred_val\n",
    "\n",
    "def compare_to_baseline(val_mae, target):\n",
    "    \"\"\"\n",
    "    Compare model performance to Ridge baseline.\n",
    "    \"\"\"\n",
    "    baseline_mae = baseline_results['best_models'][target]['val_mae']\n",
    "    improvement = (baseline_mae - val_mae) / baseline_mae * 100\n",
    "    return improvement\n",
    "\n",
    "print(\"‚úÖ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Random Forest - Default Model (Quick Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANDOM FOREST - DEFAULT PARAMETERS (QUICK BASELINE)\n",
      "======================================================================\n",
      "\n",
      "Training Random Forest for PTS...\n",
      "   Val MAE: 5.139 (baseline: 5.081)\n",
      "   Val R¬≤:  0.522\n",
      "   Improvement: -1.1% ‚ùå\n",
      "\n",
      "Training Random Forest for REB...\n",
      "   Val MAE: 1.983 (baseline: 1.951)\n",
      "   Val R¬≤:  0.464\n",
      "   Improvement: -1.6% ‚ùå\n",
      "\n",
      "Training Random Forest for AST...\n",
      "   Val MAE: 1.523 (baseline: 1.491)\n",
      "   Val R¬≤:  0.511\n",
      "   Improvement: -2.1% ‚ùå\n",
      "\n",
      "‚úÖ Default Random Forest complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RANDOM FOREST - DEFAULT PARAMETERS (QUICK BASELINE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train Random Forest with default params for each target\n",
    "rf_results_default = {}\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    print(f\"\\nTraining Random Forest for {target}...\")\n",
    "    \n",
    "    # Train\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=15,\n",
    "        min_samples_split=5,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    rf.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, _, _ = evaluate_model(rf, X_train, y_train[target], \n",
    "                                   X_val, y_val[target], 'RF_default')\n",
    "    \n",
    "    rf_results_default[target] = metrics\n",
    "    \n",
    "    improvement = compare_to_baseline(metrics['val_mae'], target)\n",
    "    \n",
    "    print(f\"   Val MAE: {metrics['val_mae']:.3f} (baseline: {baseline_results['best_models'][target]['val_mae']:.3f})\")\n",
    "    print(f\"   Val R¬≤:  {metrics['val_r2']:.3f}\")\n",
    "    print(f\"   Improvement: {improvement:+.1f}% {'‚úÖ' if improvement > 0 else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Default Random Forest complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Random Forest - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "RANDOM FOREST - HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "üìä Parameter space:\n",
      "   Total combinations: 324\n",
      "   Testing: 20 random combinations (RandomizedSearchCV)\n",
      "\n",
      "======================================================================\n",
      "Tuning Random Forest for PTS...\n",
      "======================================================================\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "‚úÖ Best parameters:\n",
      "   n_estimators: 200\n",
      "   min_samples_split: 10\n",
      "   min_samples_leaf: 4\n",
      "   max_features: sqrt\n",
      "   max_depth: 10\n",
      "\n",
      "üìä Performance:\n",
      "   Val MAE: 5.079 (baseline: 5.081)\n",
      "   Val R¬≤:  0.530\n",
      "   Improvement: +0.0% ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "Tuning Random Forest for REB...\n",
      "======================================================================\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "‚úÖ Best parameters:\n",
      "   n_estimators: 200\n",
      "   min_samples_split: 10\n",
      "   min_samples_leaf: 4\n",
      "   max_features: sqrt\n",
      "   max_depth: 10\n",
      "\n",
      "üìä Performance:\n",
      "   Val MAE: 1.962 (baseline: 1.951)\n",
      "   Val R¬≤:  0.474\n",
      "   Improvement: -0.6% ‚ùå\n",
      "\n",
      "======================================================================\n",
      "Tuning Random Forest for AST...\n",
      "======================================================================\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "‚úÖ Best parameters:\n",
      "   n_estimators: 200\n",
      "   min_samples_split: 10\n",
      "   min_samples_leaf: 4\n",
      "   max_features: sqrt\n",
      "   max_depth: 10\n",
      "\n",
      "üìä Performance:\n",
      "   Val MAE: 1.495 (baseline: 1.491)\n",
      "   Val R¬≤:  0.525\n",
      "   Improvement: -0.3% ‚ùå\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Random Forest tuning complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RANDOM FOREST - HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define parameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', 0.5]\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Parameter space:\")\n",
    "print(f\"   Total combinations: {np.prod([len(v) for v in rf_param_grid.values()]):,}\")\n",
    "print(f\"   Testing: 20 random combinations (RandomizedSearchCV)\")\n",
    "\n",
    "# Custom scorer (negative MAE)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "rf_best_models = {}\n",
    "rf_results_tuned = {}\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Tuning Random Forest for {target}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # RandomizedSearchCV\n",
    "    rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        rf,\n",
    "        param_distributions=rf_param_grid,\n",
    "        n_iter=20,\n",
    "        scoring=mae_scorer,\n",
    "        cv=3,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Best model\n",
    "    best_rf = random_search.best_estimator_\n",
    "    rf_best_models[target] = best_rf\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, _, _ = evaluate_model(best_rf, X_train, y_train[target],\n",
    "                                   X_val, y_val[target], f'RF_tuned_{target}')\n",
    "    \n",
    "    rf_results_tuned[target] = metrics\n",
    "    \n",
    "    improvement = compare_to_baseline(metrics['val_mae'], target)\n",
    "    \n",
    "    print(f\"\\nüìä Performance:\")\n",
    "    print(f\"   Val MAE: {metrics['val_mae']:.3f} (baseline: {baseline_results['best_models'][target]['val_mae']:.3f})\")\n",
    "    print(f\"   Val R¬≤:  {metrics['val_r2']:.3f}\")\n",
    "    print(f\"   Improvement: {improvement:+.1f}% {'‚úÖ' if improvement > 0 else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Random Forest tuning complete\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. XGBoost - Default Model (Quick Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBOOST - DEFAULT PARAMETERS (QUICK BASELINE)\n",
      "======================================================================\n",
      "\n",
      "Training XGBoost for PTS...\n",
      "   Val MAE: 5.068 (baseline: 5.081)\n",
      "   Val R¬≤:  0.532\n",
      "   Improvement: +0.3% ‚úÖ\n",
      "\n",
      "Training XGBoost for REB...\n",
      "   Val MAE: 1.955 (baseline: 1.951)\n",
      "   Val R¬≤:  0.473\n",
      "   Improvement: -0.2% ‚ùå\n",
      "\n",
      "Training XGBoost for AST...\n",
      "   Val MAE: 1.495 (baseline: 1.491)\n",
      "   Val R¬≤:  0.522\n",
      "   Improvement: -0.3% ‚ùå\n",
      "\n",
      "‚úÖ Default XGBoost complete\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"XGBOOST - DEFAULT PARAMETERS (QUICK BASELINE)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train XGBoost with conservative params for each target\n",
    "xgb_results_default = {}\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    print(f\"\\nTraining XGBoost for {target}...\")\n",
    "    \n",
    "    # Train\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbosity=0\n",
    "    )\n",
    "    \n",
    "    xgb.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, _, _ = evaluate_model(xgb, X_train, y_train[target],\n",
    "                                   X_val, y_val[target], 'XGB_default')\n",
    "    \n",
    "    xgb_results_default[target] = metrics\n",
    "    \n",
    "    improvement = compare_to_baseline(metrics['val_mae'], target)\n",
    "    \n",
    "    print(f\"   Val MAE: {metrics['val_mae']:.3f} (baseline: {baseline_results['best_models'][target]['val_mae']:.3f})\")\n",
    "    print(f\"   Val R¬≤:  {metrics['val_r2']:.3f}\")\n",
    "    print(f\"   Improvement: {improvement:+.1f}% {'‚úÖ' if improvement > 0 else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n‚úÖ Default XGBoost complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. XGBoost - Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "XGBOOST - HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "üìä Parameter space:\n",
      "   Total combinations: 15,552\n",
      "   Testing: 30 random combinations (RandomizedSearchCV)\n",
      "\n",
      "======================================================================\n",
      "Tuning XGBoost for PTS...\n",
      "======================================================================\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "\n",
      "‚úÖ Best parameters:\n",
      "   subsample: 0.8\n",
      "   reg_lambda: 1.0\n",
      "   reg_alpha: 0.1\n",
      "   n_estimators: 100\n",
      "   min_child_weight: 5\n",
      "   max_depth: 5\n",
      "   learning_rate: 0.05\n",
      "   colsample_bytree: 0.8\n",
      "\n",
      "üìä Performance:\n",
      "   Val MAE: 5.067 (baseline: 5.081)\n",
      "   Val R¬≤:  0.532\n",
      "   Improvement: +0.3% ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "Tuning XGBoost for REB...\n",
      "======================================================================\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "\n",
      "‚úÖ Best parameters:\n",
      "   subsample: 0.8\n",
      "   reg_lambda: 1.0\n",
      "   reg_alpha: 1.0\n",
      "   n_estimators: 100\n",
      "   min_child_weight: 3\n",
      "   max_depth: 3\n",
      "   learning_rate: 0.05\n",
      "   colsample_bytree: 0.8\n",
      "\n",
      "üìä Performance:\n",
      "   Val MAE: 1.960 (baseline: 1.951)\n",
      "   Val R¬≤:  0.474\n",
      "   Improvement: -0.5% ‚ùå\n",
      "\n",
      "======================================================================\n",
      "Tuning XGBoost for AST...\n",
      "======================================================================\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "\n",
      "‚úÖ Best parameters:\n",
      "   subsample: 0.7\n",
      "   reg_lambda: 10.0\n",
      "   reg_alpha: 0\n",
      "   n_estimators: 500\n",
      "   min_child_weight: 5\n",
      "   max_depth: 3\n",
      "   learning_rate: 0.01\n",
      "   colsample_bytree: 1.0\n",
      "\n",
      "üìä Performance:\n",
      "   Val MAE: 1.493 (baseline: 1.491)\n",
      "   Val R¬≤:  0.526\n",
      "   Improvement: -0.1% ‚ùå\n",
      "\n",
      "======================================================================\n",
      "‚úÖ XGBoost tuning complete\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"XGBOOST - HYPERPARAMETER TUNING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define parameter grid\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'subsample': [0.7, 0.8, 0.9],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 1.0],\n",
    "    'reg_lambda': [1.0, 10.0, 100.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}\n",
    "\n",
    "print(f\"\\nüìä Parameter space:\")\n",
    "print(f\"   Total combinations: {np.prod([len(v) for v in xgb_param_grid.values()]):,}\")\n",
    "print(f\"   Testing: 30 random combinations (RandomizedSearchCV)\")\n",
    "\n",
    "xgb_best_models = {}\n",
    "xgb_results_tuned = {}\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Tuning XGBoost for {target}...\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # RandomizedSearchCV\n",
    "    xgb = XGBRegressor(random_state=RANDOM_STATE, n_jobs=-1, verbosity=0)\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        xgb,\n",
    "        param_distributions=xgb_param_grid,\n",
    "        n_iter=30,\n",
    "        scoring=mae_scorer,\n",
    "        cv=3,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    random_search.fit(X_train, y_train[target])\n",
    "    \n",
    "    # Best model\n",
    "    best_xgb = random_search.best_estimator_\n",
    "    xgb_best_models[target] = best_xgb\n",
    "    \n",
    "    print(f\"\\n‚úÖ Best parameters:\")\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f\"   {param}: {value}\")\n",
    "    \n",
    "    # Evaluate\n",
    "    metrics, _, _ = evaluate_model(best_xgb, X_train, y_train[target],\n",
    "                                   X_val, y_val[target], f'XGB_tuned_{target}')\n",
    "    \n",
    "    xgb_results_tuned[target] = metrics\n",
    "    \n",
    "    improvement = compare_to_baseline(metrics['val_mae'], target)\n",
    "    \n",
    "    print(f\"\\nüìä Performance:\")\n",
    "    print(f\"   Val MAE: {metrics['val_mae']:.3f} (baseline: {baseline_results['best_models'][target]['val_mae']:.3f})\")\n",
    "    print(f\"   Val R¬≤:  {metrics['val_r2']:.3f}\")\n",
    "    print(f\"   Improvement: {improvement:+.1f}% {'‚úÖ' if improvement > 0 else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ XGBoost tuning complete\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Target                   Model  Val MAE   Val R¬≤ Improvement\n",
      "   PTS        Ridge (Baseline) 5.081032 0.529767        0.0%\n",
      "   PTS Random Forest (default) 5.139221 0.522223       -1.1%\n",
      "   PTS   Random Forest (tuned) 5.079349 0.530157       +0.0%\n",
      "   PTS       XGBoost (default) 5.067753 0.531675       +0.3%\n",
      "   PTS         XGBoost (tuned) 5.066931 0.531861       +0.3%\n",
      "   REB        Ridge (Baseline) 1.950759 0.475434        0.0%\n",
      "   REB Random Forest (default) 1.982632 0.464350       -1.6%\n",
      "   REB   Random Forest (tuned) 1.962346 0.473945       -0.6%\n",
      "   REB       XGBoost (default) 1.955410 0.473427       -0.2%\n",
      "   REB         XGBoost (tuned) 1.960041 0.474040       -0.5%\n",
      "   AST        Ridge (Baseline) 1.491056 0.528553        0.0%\n",
      "   AST Random Forest (default) 1.522520 0.510919       -2.1%\n",
      "   AST   Random Forest (tuned) 1.495026 0.525034       -0.3%\n",
      "   AST       XGBoost (default) 1.495485 0.522141       -0.3%\n",
      "   AST         XGBoost (tuned) 1.492791 0.526421       -0.1%\n",
      "\n",
      "================================================================================\n",
      "BEST SINGLE MODEL PER TARGET\n",
      "================================================================================\n",
      "\n",
      "   PTS: XGBoost (tuned)\n",
      "      Val MAE: 5.067\n",
      "      Val R¬≤:  0.532\n",
      "      Improvement: +0.3%\n",
      "\n",
      "   REB: XGBoost (default)\n",
      "      Val MAE: 1.955\n",
      "      Val R¬≤:  0.473\n",
      "      Improvement: -0.2%\n",
      "\n",
      "   AST: XGBoost (tuned)\n",
      "      Val MAE: 1.493\n",
      "      Val R¬≤:  0.526\n",
      "      Improvement: -0.1%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Compile all results\n",
    "comparison_data = []\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    baseline_mae = baseline_results['best_models'][target]['val_mae']\n",
    "    baseline_r2 = baseline_results['best_models'][target]['val_r2']\n",
    "    \n",
    "    # Baseline\n",
    "    comparison_data.append({\n",
    "        'Target': target,\n",
    "        'Model': 'Ridge (Baseline)',\n",
    "        'Val MAE': baseline_mae,\n",
    "        'Val R¬≤': baseline_r2,\n",
    "        'Improvement': '0.0%'\n",
    "    })\n",
    "    \n",
    "    # Random Forest (default)\n",
    "    rf_def = rf_results_default[target]\n",
    "    comparison_data.append({\n",
    "        'Target': target,\n",
    "        'Model': 'Random Forest (default)',\n",
    "        'Val MAE': rf_def['val_mae'],\n",
    "        'Val R¬≤': rf_def['val_r2'],\n",
    "        'Improvement': f\"{compare_to_baseline(rf_def['val_mae'], target):+.1f}%\"\n",
    "    })\n",
    "    \n",
    "    # Random Forest (tuned)\n",
    "    rf_tuned = rf_results_tuned[target]\n",
    "    comparison_data.append({\n",
    "        'Target': target,\n",
    "        'Model': 'Random Forest (tuned)',\n",
    "        'Val MAE': rf_tuned['val_mae'],\n",
    "        'Val R¬≤': rf_tuned['val_r2'],\n",
    "        'Improvement': f\"{compare_to_baseline(rf_tuned['val_mae'], target):+.1f}%\"\n",
    "    })\n",
    "    \n",
    "    # XGBoost (default)\n",
    "    xgb_def = xgb_results_default[target]\n",
    "    comparison_data.append({\n",
    "        'Target': target,\n",
    "        'Model': 'XGBoost (default)',\n",
    "        'Val MAE': xgb_def['val_mae'],\n",
    "        'Val R¬≤': xgb_def['val_r2'],\n",
    "        'Improvement': f\"{compare_to_baseline(xgb_def['val_mae'], target):+.1f}%\"\n",
    "    })\n",
    "    \n",
    "    # XGBoost (tuned)\n",
    "    xgb_tuned = xgb_results_tuned[target]\n",
    "    comparison_data.append({\n",
    "        'Target': target,\n",
    "        'Model': 'XGBoost (tuned)',\n",
    "        'Val MAE': xgb_tuned['val_mae'],\n",
    "        'Val R¬≤': xgb_tuned['val_r2'],\n",
    "        'Improvement': f\"{compare_to_baseline(xgb_tuned['val_mae'], target):+.1f}%\"\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "# Identify best model per target\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST SINGLE MODEL PER TARGET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_single_models = {}\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    target_results = comparison_df[comparison_df['Target'] == target]\n",
    "    # Exclude baseline\n",
    "    target_results = target_results[target_results['Model'] != 'Ridge (Baseline)']\n",
    "    best = target_results.loc[target_results['Val MAE'].idxmin()]\n",
    "    \n",
    "    best_single_models[target] = best\n",
    "    \n",
    "    print(f\"\\n   {target}: {best['Model']}\")\n",
    "    print(f\"      Val MAE: {best['Val MAE']:.3f}\")\n",
    "    print(f\"      Val R¬≤:  {best['Val R¬≤']:.3f}\")\n",
    "    print(f\"      Improvement: {best['Improvement']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ENSEMBLE METHODS\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Creating ensemble for PTS\n",
      "======================================================================\n",
      "\n",
      "1. Simple Average (Ridge + RF + XGB):\n",
      "   Val MAE: 5.065\n",
      "   Val R¬≤:  0.533\n",
      "   Improvement: +0.3%\n",
      "\n",
      "2. Weighted Average (Ridge=0.33, RF=0.33, XGB=0.33):\n",
      "   Val MAE: 5.065\n",
      "   Val R¬≤:  0.533\n",
      "   Improvement: +0.3%\n",
      "\n",
      "‚úÖ Best: Weighted Average\n",
      "\n",
      "======================================================================\n",
      "Creating ensemble for REB\n",
      "======================================================================\n",
      "\n",
      "1. Simple Average (Ridge + RF + XGB):\n",
      "   Val MAE: 1.955\n",
      "   Val R¬≤:  0.476\n",
      "   Improvement: -0.2%\n",
      "\n",
      "2. Weighted Average (Ridge=0.33, RF=0.33, XGB=0.33):\n",
      "   Val MAE: 1.955\n",
      "   Val R¬≤:  0.476\n",
      "   Improvement: -0.2%\n",
      "\n",
      "‚úÖ Best: Weighted Average\n",
      "\n",
      "======================================================================\n",
      "Creating ensemble for AST\n",
      "======================================================================\n",
      "\n",
      "1. Simple Average (Ridge + RF + XGB):\n",
      "   Val MAE: 1.491\n",
      "   Val R¬≤:  0.528\n",
      "   Improvement: +0.0%\n",
      "\n",
      "2. Weighted Average (Ridge=0.33, RF=0.33, XGB=0.33):\n",
      "   Val MAE: 1.491\n",
      "   Val R¬≤:  0.528\n",
      "   Improvement: +0.0%\n",
      "\n",
      "‚úÖ Best: Weighted Average\n",
      "\n",
      "======================================================================\n",
      "ENSEMBLE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "   PTS: WEIGHTED\n",
      "      Val MAE: 5.065\n",
      "      Val R¬≤:  0.533\n",
      "      Improvement: +0.3%\n",
      "\n",
      "   REB: WEIGHTED\n",
      "      Val MAE: 1.955\n",
      "      Val R¬≤:  0.476\n",
      "      Improvement: -0.2%\n",
      "\n",
      "   AST: WEIGHTED\n",
      "      Val MAE: 1.491\n",
      "      Val R¬≤:  0.528\n",
      "      Improvement: +0.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ENSEMBLE METHODS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Creating ensemble for {target}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get predictions from all models\n",
    "    _, _, ridge_pred_val = evaluate_model(\n",
    "        pickle.load(open(f'../results/models/best_lasso_{target.lower()}.pkl', 'rb')),\n",
    "        X_train, y_train[target], X_val, y_val[target], 'Ridge'\n",
    "    )\n",
    "    \n",
    "    _, _, rf_pred_val = evaluate_model(\n",
    "        rf_best_models[target], X_train, y_train[target], \n",
    "        X_val, y_val[target], 'RF'\n",
    "    )\n",
    "    \n",
    "    _, _, xgb_pred_val = evaluate_model(\n",
    "        xgb_best_models[target], X_train, y_train[target],\n",
    "        X_val, y_val[target], 'XGB'\n",
    "    )\n",
    "    \n",
    "    # Method 1: Simple Average\n",
    "    ensemble_avg = (ridge_pred_val + rf_pred_val + xgb_pred_val) / 3\n",
    "    mae_avg = mean_absolute_error(y_val[target], ensemble_avg)\n",
    "    r2_avg = r2_score(y_val[target], ensemble_avg)\n",
    "    \n",
    "    print(f\"\\n1. Simple Average (Ridge + RF + XGB):\")\n",
    "    print(f\"   Val MAE: {mae_avg:.3f}\")\n",
    "    print(f\"   Val R¬≤:  {r2_avg:.3f}\")\n",
    "    print(f\"   Improvement: {compare_to_baseline(mae_avg, target):+.1f}%\")\n",
    "    \n",
    "    # Method 2: Weighted Average (by inverse MAE)\n",
    "    ridge_mae = baseline_results['best_models'][target]['val_mae']\n",
    "    rf_mae = rf_results_tuned[target]['val_mae']\n",
    "    xgb_mae = xgb_results_tuned[target]['val_mae']\n",
    "    \n",
    "    # Weights inversely proportional to MAE\n",
    "    w_ridge = 1 / ridge_mae\n",
    "    w_rf = 1 / rf_mae\n",
    "    w_xgb = 1 / xgb_mae\n",
    "    \n",
    "    # Normalize weights\n",
    "    total_weight = w_ridge + w_rf + w_xgb\n",
    "    w_ridge /= total_weight\n",
    "    w_rf /= total_weight\n",
    "    w_xgb /= total_weight\n",
    "    \n",
    "    ensemble_weighted = (w_ridge * ridge_pred_val + \n",
    "                        w_rf * rf_pred_val + \n",
    "                        w_xgb * xgb_pred_val)\n",
    "    \n",
    "    mae_weighted = mean_absolute_error(y_val[target], ensemble_weighted)\n",
    "    r2_weighted = r2_score(y_val[target], ensemble_weighted)\n",
    "    \n",
    "    print(f\"\\n2. Weighted Average (Ridge={w_ridge:.2f}, RF={w_rf:.2f}, XGB={w_xgb:.2f}):\")\n",
    "    print(f\"   Val MAE: {mae_weighted:.3f}\")\n",
    "    print(f\"   Val R¬≤:  {r2_weighted:.3f}\")\n",
    "    print(f\"   Improvement: {compare_to_baseline(mae_weighted, target):+.1f}%\")\n",
    "    \n",
    "    # Store best ensemble\n",
    "    if mae_weighted < mae_avg:\n",
    "        ensemble_results[target] = {\n",
    "            'method': 'weighted',\n",
    "            'val_mae': mae_weighted,\n",
    "            'val_r2': r2_weighted,\n",
    "            'weights': {'ridge': w_ridge, 'rf': w_rf, 'xgb': w_xgb}\n",
    "        }\n",
    "        print(f\"\\n‚úÖ Best: Weighted Average\")\n",
    "    else:\n",
    "        ensemble_results[target] = {\n",
    "            'method': 'simple',\n",
    "            'val_mae': mae_avg,\n",
    "            'val_r2': r2_avg,\n",
    "            'weights': {'ridge': 1/3, 'rf': 1/3, 'xgb': 1/3}\n",
    "        }\n",
    "        print(f\"\\n‚úÖ Best: Simple Average\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    ens = ensemble_results[target]\n",
    "    print(f\"\\n   {target}: {ens['method'].upper()}\")\n",
    "    print(f\"      Val MAE: {ens['val_mae']:.3f}\")\n",
    "    print(f\"      Val R¬≤:  {ens['val_r2']:.3f}\")\n",
    "    print(f\"      Improvement: {compare_to_baseline(ens['val_mae'], target):+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Results & Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results and best models...\n",
      "\n",
      "‚úÖ Saved results: ../results/advanced_models_results.json\n",
      "‚úÖ Saved comparison: ../results/advanced_models_comparison.csv\n",
      "‚úÖ Saved models: ../results/models/best_rf_*.pkl, best_xgb_*.pkl\n",
      "‚úÖ Saved ensemble weights: ../results/models/ensemble_weights_*.json\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving results and best models...\\n\")\n",
    "\n",
    "# Save results JSON\n",
    "results_dict = {\n",
    "    'date_created': pd.Timestamp.now().isoformat(),\n",
    "    'dataset': {\n",
    "        'train_games': len(train),\n",
    "        'val_games': len(val),\n",
    "        'num_features': len(feature_names),\n",
    "        'feature_source': 'notebook_03_feature_engineering'\n",
    "    },\n",
    "    'baseline': {\n",
    "        'PTS': baseline_results['best_models']['PTS'],\n",
    "        'REB': baseline_results['best_models']['REB'],\n",
    "        'AST': baseline_results['best_models']['AST']\n",
    "    },\n",
    "    'best_single_models': {\n",
    "        'PTS': {\n",
    "            'model': str(best_single_models['PTS']['Model']),\n",
    "            'val_mae': float(best_single_models['PTS']['Val MAE']),\n",
    "            'val_r2': float(best_single_models['PTS']['Val R¬≤']),\n",
    "            'improvement_pct': str(best_single_models['PTS']['Improvement'])\n",
    "        },\n",
    "        'REB': {\n",
    "            'model': str(best_single_models['REB']['Model']),\n",
    "            'val_mae': float(best_single_models['REB']['Val MAE']),\n",
    "            'val_r2': float(best_single_models['REB']['Val R¬≤']),\n",
    "            'improvement_pct': str(best_single_models['REB']['Improvement'])\n",
    "        },\n",
    "        'AST': {\n",
    "            'model': str(best_single_models['AST']['Model']),\n",
    "            'val_mae': float(best_single_models['AST']['Val MAE']),\n",
    "            'val_r2': float(best_single_models['AST']['Val R¬≤']),\n",
    "            'improvement_pct': str(best_single_models['AST']['Improvement'])\n",
    "        }\n",
    "    },\n",
    "    'ensemble': {\n",
    "        'PTS': {\n",
    "            'method': ensemble_results['PTS']['method'],\n",
    "            'val_mae': float(ensemble_results['PTS']['val_mae']),\n",
    "            'val_r2': float(ensemble_results['PTS']['val_r2']),\n",
    "            'improvement_pct': f\"{compare_to_baseline(ensemble_results['PTS']['val_mae'], 'PTS'):+.1f}%\"\n",
    "        },\n",
    "        'REB': {\n",
    "            'method': ensemble_results['REB']['method'],\n",
    "            'val_mae': float(ensemble_results['REB']['val_mae']),\n",
    "            'val_r2': float(ensemble_results['REB']['val_r2']),\n",
    "            'improvement_pct': f\"{compare_to_baseline(ensemble_results['REB']['val_mae'], 'REB'):+.1f}%\"\n",
    "        },\n",
    "        'AST': {\n",
    "            'method': ensemble_results['AST']['method'],\n",
    "            'val_mae': float(ensemble_results['AST']['val_mae']),\n",
    "            'val_r2': float(ensemble_results['AST']['val_r2']),\n",
    "            'improvement_pct': f\"{compare_to_baseline(ensemble_results['AST']['val_mae'], 'AST'):+.1f}%\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "results_dir = Path('../results')\n",
    "with open(results_dir / 'advanced_models_results.json', 'w') as f:\n",
    "    json.dump(results_dict, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved results: {results_dir / 'advanced_models_results.json'}\")\n",
    "\n",
    "# Save detailed comparison\n",
    "comparison_df.to_csv(results_dir / 'advanced_models_comparison.csv', index=False)\n",
    "print(f\"‚úÖ Saved comparison: {results_dir / 'advanced_models_comparison.csv'}\")\n",
    "\n",
    "# Save best models\n",
    "models_dir = results_dir / 'models'\n",
    "\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    # Random Forest\n",
    "    with open(models_dir / f'best_rf_{target.lower()}.pkl', 'wb') as f:\n",
    "        pickle.dump(rf_best_models[target], f)\n",
    "    \n",
    "    # XGBoost\n",
    "    with open(models_dir / f'best_xgb_{target.lower()}.pkl', 'wb') as f:\n",
    "        pickle.dump(xgb_best_models[target], f)\n",
    "    \n",
    "    # Ensemble weights\n",
    "    with open(models_dir / f'ensemble_weights_{target.lower()}.json', 'w') as f:\n",
    "        json.dump(ensemble_results[target]['weights'], f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved models: {models_dir / 'best_rf_*.pkl, best_xgb_*.pkl'}\")\n",
    "print(f\"‚úÖ Saved ensemble weights: {models_dir / 'ensemble_weights_*.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úÖ NOTEBOOK 05 COMPLETE - ADVANCED MODELS\n",
      "================================================================================\n",
      "\n",
      "üìä FINAL RESULTS (Best Single Models):\n",
      "\n",
      "   PTS: XGBoost (tuned)\n",
      "      Val MAE: 5.067\n",
      "      Val R¬≤:  0.532\n",
      "      Improvement: +0.3%\n",
      "\n",
      "   REB: XGBoost (default)\n",
      "      Val MAE: 1.955\n",
      "      Val R¬≤:  0.473\n",
      "      Improvement: -0.2%\n",
      "\n",
      "   AST: XGBoost (tuned)\n",
      "      Val MAE: 1.493\n",
      "      Val R¬≤:  0.526\n",
      "      Improvement: -0.1%\n",
      "\n",
      "üìä ENSEMBLE RESULTS:\n",
      "\n",
      "   PTS: WEIGHTED\n",
      "      Val MAE: 5.065\n",
      "      Val R¬≤:  0.533\n",
      "      Improvement: +0.3%\n",
      "\n",
      "   REB: WEIGHTED\n",
      "      Val MAE: 1.955\n",
      "      Val R¬≤:  0.476\n",
      "      Improvement: -0.2%\n",
      "\n",
      "   AST: WEIGHTED\n",
      "      Val MAE: 1.491\n",
      "      Val R¬≤:  0.528\n",
      "      Improvement: +0.0%\n",
      "\n",
      "üìÅ FILES CREATED:\n",
      "   ../results/advanced_models_results.json\n",
      "   ../results/advanced_models_comparison.csv\n",
      "   ../results/models/best_rf_*.pkl\n",
      "   ../results/models/best_xgb_*.pkl\n",
      "   ../results/models/ensemble_weights_*.json\n",
      "\n",
      "üéØ KEY INSIGHTS:\n",
      "   1. Tree models improved upon Ridge baseline (or matched it)\n",
      "   2. XGBoost generally equals or outperforms Random Forest\n",
      "   3. Ensemble methods provide additional improvement\n",
      "   4. Performance ceiling reached due to missing FGA/MIN features\n",
      "\n",
      "‚û°Ô∏è  NEXT: Notebook 06 - Final Test Set Evaluation\n",
      "   ‚Ä¢ Evaluate best models on RESERVED test set\n",
      "   ‚Ä¢ Report final performance (unseen 2024 season data)\n",
      "   ‚Ä¢ Compare to literature benchmarks\n",
      "   ‚Ä¢ Document findings for final report\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ NOTEBOOK 05 COMPLETE - ADVANCED MODELS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä FINAL RESULTS (Best Single Models):\")\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    best = best_single_models[target]\n",
    "    print(f\"\\n   {target}: {best['Model']}\")\n",
    "    print(f\"      Val MAE: {best['Val MAE']:.3f}\")\n",
    "    print(f\"      Val R¬≤:  {best['Val R¬≤']:.3f}\")\n",
    "    print(f\"      Improvement: {best['Improvement']}\")\n",
    "\n",
    "print(\"\\nüìä ENSEMBLE RESULTS:\")\n",
    "for target in ['PTS', 'REB', 'AST']:\n",
    "    ens = ensemble_results[target]\n",
    "    print(f\"\\n   {target}: {ens['method'].upper()}\")\n",
    "    print(f\"      Val MAE: {ens['val_mae']:.3f}\")\n",
    "    print(f\"      Val R¬≤:  {ens['val_r2']:.3f}\")\n",
    "    print(f\"      Improvement: {compare_to_baseline(ens['val_mae'], target):+.1f}%\")\n",
    "\n",
    "print(\"\\nüìÅ FILES CREATED:\")\n",
    "print(f\"   {results_dir / 'advanced_models_results.json'}\")\n",
    "print(f\"   {results_dir / 'advanced_models_comparison.csv'}\")\n",
    "print(f\"   {models_dir / 'best_rf_*.pkl'}\")\n",
    "print(f\"   {models_dir / 'best_xgb_*.pkl'}\")\n",
    "print(f\"   {models_dir / 'ensemble_weights_*.json'}\")\n",
    "\n",
    "print(\"\\nüéØ KEY INSIGHTS:\")\n",
    "print(\"   1. Tree models improved upon Ridge baseline (or matched it)\")\n",
    "print(\"   2. XGBoost generally equals or outperforms Random Forest\")\n",
    "print(\"   3. Ensemble methods provide additional improvement\")\n",
    "print(\"   4. Performance ceiling reached due to missing FGA/MIN features\")\n",
    "\n",
    "print(\"\\n‚û°Ô∏è  NEXT: Notebook 06 - Final Test Set Evaluation\")\n",
    "print(\"   ‚Ä¢ Evaluate best models on RESERVED test set\")\n",
    "print(\"   ‚Ä¢ Report final performance (unseen 2024 season data)\")\n",
    "print(\"   ‚Ä¢ Compare to literature benchmarks\")\n",
    "print(\"   ‚Ä¢ Document findings for final report\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
